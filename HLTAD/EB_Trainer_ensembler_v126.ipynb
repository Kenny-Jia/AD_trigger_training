{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4db7479-2e95-49b5-afd6-177cecaa3330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 20:35:15.360753: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-30 20:35:15.412256: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.30/04\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import h5py\n",
    "#import random\n",
    "#import sklearn\n",
    "#import collections\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#import json\n",
    "#import pylab \n",
    "#from scipy.optimize import curve_fit\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "#import shap\n",
    "#import pandas as pd\n",
    "import tensorflow as tf\n",
    "#import tarfile\n",
    "from tensorflow.keras.models import load_model\n",
    "#from qkeras import QActivation, QDense, QConv2D, QBatchNormalization\n",
    "import ensembler_functions as ef\n",
    "import tf2onnx\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86a2b96d-16e4-43ac-af26-5b123306af79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set matplotlib default color cycle\n",
    "new_color_cycle = [\n",
    "    '#1f77b4',\n",
    "    '#ff7f0e',\n",
    "    '#2ca02c',\n",
    "    '#d62728',\n",
    "    '#9467bd',\n",
    "    '#8c564b',\n",
    "    '#e377c2',\n",
    "    '#7f7f7f',\n",
    "    '#bcbd22',\n",
    "    '#17becf',\n",
    "    '#aec7e8',\n",
    "    '#ffbb78',\n",
    "    '#98df8a',\n",
    "    '#ff9896',\n",
    "    '#c5b0d5',\n",
    "    '#c49c94',\n",
    "    '#f7b6d2',\n",
    "    '#c7c7c7',\n",
    "    '#dbdb8d',\n",
    "    '#9edae5'\n",
    "]\n",
    "\n",
    "# You can then apply this new color cycle to your matplotlib plots\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=new_color_cycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b53f2bf9-443e-4302-aab7-bfd6c88fade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1AD_rate = 1000\n",
    "target_rate = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67b4ae0-23f1-4730-ae7a-c430e6db034a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24a28d9f-84c3-4426-8d2c-a94ee533fd06",
   "metadata": {},
   "source": [
    "### Test standard scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c6bf04-80e5-4f4b-a5e2-3811ad2ac00f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Test ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b26414c2-21d0-4cf1-a476-b2a53faa437f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not search for non-variable resources. Concrete function internal representation may have changed.\n",
      "2024-10-30 15:45:26.715362: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2024-10-30 15:45:26.715559: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2024-10-30 15:45:27.230193: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2024-10-30 15:45:27.230453: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX HLT_AE model saved to: ./trained_models/multiple_trainings/trial_2/onnx/HLT_AE_0.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not search for non-variable resources. Concrete function internal representation may have changed.\n",
      "2024-10-30 15:45:29.812788: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2024-10-30 15:45:29.812992: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n",
      "2024-10-30 15:45:30.345443: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2024-10-30 15:45:30.345661: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX L1_AE model saved to: ./trained_models/multiple_trainings/trial_2/onnx/L1_AE_0.onnx\n",
      "Loaded A14N23LO from ./h5_ntuples/A14N23LO.h5\n",
      "Loaded EB from ./h5_ntuples/EB.h5\n",
      "Loaded EB_test2 from ./h5_ntuples/EB_test2.h5\n",
      "Loaded EB_train from ./h5_ntuples/EB_train.h5\n",
      "Loaded HAHMggfZdZd2l2nu from ./h5_ntuples/HAHMggfZdZd2l2nu.h5\n",
      "Loaded HHbbttHadHad from ./h5_ntuples/HHbbttHadHad.h5\n",
      "Loaded HLT_noalg_eb_L1All from ./h5_ntuples/HLT_noalg_eb_L1All.h5\n",
      "Loaded ZZ4lep from ./h5_ntuples/ZZ4lep.h5\n",
      "Loaded Zprime2EJs from ./h5_ntuples/Zprime2EJs.h5\n",
      "Loaded jjJZ1 from ./h5_ntuples/jjJZ1.h5\n",
      "Loaded jjJZ2 from ./h5_ntuples/jjJZ2.h5\n",
      "Loaded jjJZ4 from ./h5_ntuples/jjJZ4.h5\n",
      "Loaded qqa from ./h5_ntuples/qqa.h5\n"
     ]
    }
   ],
   "source": [
    "training_info = {\n",
    "    \"save_path\": \"./trained_models/multiple_trainings/trial_2\", \n",
    "    \"dropout_p\": 0.1, \n",
    "    \"L2_reg_coupling\": 0.01, \n",
    "    \"latent_dim\": 4, \n",
    "    \"large_network\": True, \n",
    "    \"num_trainings\": 10,\n",
    "    \"training_weights\": True\n",
    "}\n",
    "\n",
    "data_info = {\n",
    "    \"train_data_scheme\": \n",
    "    \"topo2A_train+overlap\", \n",
    "    \"pt_normalization_type\": \n",
    "    \"global_division\", \n",
    "    \"L1AD_rate\": 1000\n",
    "}\n",
    "model_version=0\n",
    "ef.convert_to_onnx(training_info=training_info, model_version=model_version, object_type='HLT', save_dir='./trained_models/multiple_trainings/trial_2/onnx')\n",
    "ef.convert_to_onnx(training_info=training_info, model_version=model_version, object_type='L1', save_dir='./trained_models/multiple_trainings/trial_2/onnx')\n",
    "datasets, data_info = ef.load_and_preprocess(**data_info)\n",
    "datasets = ef.compare_tf_with_onnx(datasets=datasets, training_info=training_info, model_version=0, onnx_path='./trained_models/multiple_trainings/trial_2/onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "328a96a8-96f2-4714-b7ce-079282e725b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A14N23LO:\n",
      "    HLT_data: (10000, 48)\n",
      "    L1_data: (10000, 48)\n",
      "    passHLT: (10000,)\n",
      "    passL1: (10000,)\n",
      "    topo2A_AD_scores: (10000,)\n",
      "    weights: (10000,)\n",
      "    L1Seeded: (10000,)\n",
      "    HLT_model_outputs: (10000, 48)\n",
      "    L1_model_outputs: (10000, 48)\n",
      "    HLT_AD_scores: (10000,)\n",
      "    L1_AD_scores: (10000,)\n",
      "    ONNX_HLT_model_outputs: (10000, 48)\n",
      "    ONNX_L1_model_outputs: (10000, 48)\n",
      "    ONNX_HLT_AD_scores: (10000,)\n",
      "    ONNX_L1_AD_scores: (10000,)\n",
      "HAHMggfZdZd2l2nu:\n",
      "    HLT_data: (70000, 48)\n",
      "    L1_data: (70000, 48)\n",
      "    passHLT: (70000,)\n",
      "    passL1: (70000,)\n",
      "    topo2A_AD_scores: (70000,)\n",
      "    weights: (70000,)\n",
      "    L1Seeded: (70000,)\n",
      "    HLT_model_outputs: (70000, 48)\n",
      "    L1_model_outputs: (70000, 48)\n",
      "    HLT_AD_scores: (70000,)\n",
      "    L1_AD_scores: (70000,)\n",
      "    ONNX_HLT_model_outputs: (70000, 48)\n",
      "    ONNX_L1_model_outputs: (70000, 48)\n",
      "    ONNX_HLT_AD_scores: (70000,)\n",
      "    ONNX_L1_AD_scores: (70000,)\n",
      "HHbbttHadHad:\n",
      "    HLT_data: (100000, 48)\n",
      "    L1_data: (100000, 48)\n",
      "    passHLT: (100000,)\n",
      "    passL1: (100000,)\n",
      "    topo2A_AD_scores: (100000,)\n",
      "    weights: (100000,)\n",
      "    L1Seeded: (100000,)\n",
      "    HLT_model_outputs: (100000, 48)\n",
      "    L1_model_outputs: (100000, 48)\n",
      "    HLT_AD_scores: (100000,)\n",
      "    L1_AD_scores: (100000,)\n",
      "    ONNX_HLT_model_outputs: (100000, 48)\n",
      "    ONNX_L1_model_outputs: (100000, 48)\n",
      "    ONNX_HLT_AD_scores: (100000,)\n",
      "    ONNX_L1_AD_scores: (100000,)\n",
      "ZZ4lep:\n",
      "    HLT_data: (100000, 48)\n",
      "    L1_data: (100000, 48)\n",
      "    passHLT: (100000,)\n",
      "    passL1: (100000,)\n",
      "    topo2A_AD_scores: (100000,)\n",
      "    weights: (100000,)\n",
      "    L1Seeded: (100000,)\n",
      "    HLT_model_outputs: (100000, 48)\n",
      "    L1_model_outputs: (100000, 48)\n",
      "    HLT_AD_scores: (100000,)\n",
      "    L1_AD_scores: (100000,)\n",
      "    ONNX_HLT_model_outputs: (100000, 48)\n",
      "    ONNX_L1_model_outputs: (100000, 48)\n",
      "    ONNX_HLT_AD_scores: (100000,)\n",
      "    ONNX_L1_AD_scores: (100000,)\n",
      "Zprime2EJs:\n",
      "    HLT_data: (100000, 48)\n",
      "    L1_data: (100000, 48)\n",
      "    passHLT: (100000,)\n",
      "    passL1: (100000,)\n",
      "    topo2A_AD_scores: (100000,)\n",
      "    weights: (100000,)\n",
      "    L1Seeded: (100000,)\n",
      "    HLT_model_outputs: (100000, 48)\n",
      "    L1_model_outputs: (100000, 48)\n",
      "    HLT_AD_scores: (100000,)\n",
      "    L1_AD_scores: (100000,)\n",
      "    ONNX_HLT_model_outputs: (100000, 48)\n",
      "    ONNX_L1_model_outputs: (100000, 48)\n",
      "    ONNX_HLT_AD_scores: (100000,)\n",
      "    ONNX_L1_AD_scores: (100000,)\n",
      "jjJZ1:\n",
      "    HLT_data: (100000, 48)\n",
      "    L1_data: (100000, 48)\n",
      "    passHLT: (100000,)\n",
      "    passL1: (100000,)\n",
      "    topo2A_AD_scores: (100000,)\n",
      "    weights: (100000,)\n",
      "    L1Seeded: (100000,)\n",
      "    HLT_model_outputs: (100000, 48)\n",
      "    L1_model_outputs: (100000, 48)\n",
      "    HLT_AD_scores: (100000,)\n",
      "    L1_AD_scores: (100000,)\n",
      "    ONNX_HLT_model_outputs: (100000, 48)\n",
      "    ONNX_L1_model_outputs: (100000, 48)\n",
      "    ONNX_HLT_AD_scores: (100000,)\n",
      "    ONNX_L1_AD_scores: (100000,)\n",
      "jjJZ2:\n",
      "    HLT_data: (100000, 48)\n",
      "    L1_data: (100000, 48)\n",
      "    passHLT: (100000,)\n",
      "    passL1: (100000,)\n",
      "    topo2A_AD_scores: (100000,)\n",
      "    weights: (100000,)\n",
      "    L1Seeded: (100000,)\n",
      "    HLT_model_outputs: (100000, 48)\n",
      "    L1_model_outputs: (100000, 48)\n",
      "    HLT_AD_scores: (100000,)\n",
      "    L1_AD_scores: (100000,)\n",
      "    ONNX_HLT_model_outputs: (100000, 48)\n",
      "    ONNX_L1_model_outputs: (100000, 48)\n",
      "    ONNX_HLT_AD_scores: (100000,)\n",
      "    ONNX_L1_AD_scores: (100000,)\n",
      "jjJZ4:\n",
      "    HLT_data: (100000, 48)\n",
      "    L1_data: (100000, 48)\n",
      "    passHLT: (100000,)\n",
      "    passL1: (100000,)\n",
      "    topo2A_AD_scores: (100000,)\n",
      "    weights: (100000,)\n",
      "    L1Seeded: (100000,)\n",
      "    HLT_model_outputs: (100000, 48)\n",
      "    L1_model_outputs: (100000, 48)\n",
      "    HLT_AD_scores: (100000,)\n",
      "    L1_AD_scores: (100000,)\n",
      "    ONNX_HLT_model_outputs: (100000, 48)\n",
      "    ONNX_L1_model_outputs: (100000, 48)\n",
      "    ONNX_HLT_AD_scores: (100000,)\n",
      "    ONNX_L1_AD_scores: (100000,)\n",
      "qqa:\n",
      "    HLT_data: (50000, 48)\n",
      "    L1_data: (50000, 48)\n",
      "    passHLT: (50000,)\n",
      "    passL1: (50000,)\n",
      "    topo2A_AD_scores: (50000,)\n",
      "    weights: (50000,)\n",
      "    L1Seeded: (50000,)\n",
      "    HLT_model_outputs: (50000, 48)\n",
      "    L1_model_outputs: (50000, 48)\n",
      "    HLT_AD_scores: (50000,)\n",
      "    L1_AD_scores: (50000,)\n",
      "    ONNX_HLT_model_outputs: (50000, 48)\n",
      "    ONNX_L1_model_outputs: (50000, 48)\n",
      "    ONNX_HLT_AD_scores: (50000,)\n",
      "    ONNX_L1_AD_scores: (50000,)\n",
      "EB_test:\n",
      "    HLT_data: (509167, 48)\n",
      "    L1_data: (509167, 48)\n",
      "    event_numbers: (509167,)\n",
      "    passHLT: (509167,)\n",
      "    passL1: (509167,)\n",
      "    pileups: (509167,)\n",
      "    run_numbers: (509167,)\n",
      "    topo2A_AD_scores: (509167,)\n",
      "    weights: (509167,)\n",
      "    L1Seeded: (509167,)\n",
      "    HLT_model_outputs: (509167, 48)\n",
      "    L1_model_outputs: (509167, 48)\n",
      "    HLT_AD_scores: (509167,)\n",
      "    L1_AD_scores: (509167,)\n",
      "    ONNX_HLT_model_outputs: (509167, 48)\n",
      "    ONNX_L1_model_outputs: (509167, 48)\n",
      "    ONNX_HLT_AD_scores: (509167,)\n",
      "    ONNX_L1_AD_scores: (509167,)\n",
      "EB_train:\n",
      "    HLT_data: (1493152, 48)\n",
      "    L1_data: (1493152, 48)\n",
      "    event_numbers: (1493152,)\n",
      "    passHLT: (1493152,)\n",
      "    passL1: (1493152,)\n",
      "    pileups: (1493152,)\n",
      "    run_numbers: (1493152,)\n",
      "    topo2A_AD_scores: (1493152,)\n",
      "    weights: (1493152,)\n",
      "    L1Seeded: (1493152,)\n",
      "EB_val:\n",
      "    HLT_data: (263498, 48)\n",
      "    L1_data: (263498, 48)\n",
      "    event_numbers: (263498,)\n",
      "    passHLT: (263498,)\n",
      "    passL1: (263498,)\n",
      "    pileups: (263498,)\n",
      "    run_numbers: (263498,)\n",
      "    topo2A_AD_scores: (263498,)\n",
      "    weights: (263498,)\n",
      "    L1Seeded: (263498,)\n"
     ]
    }
   ],
   "source": [
    "for tag, data_dict in datasets.items():\n",
    "    print(f'{tag}:')\n",
    "    for key, value in data_dict.items():\n",
    "        print(f'    {key}: {value.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5f166e0-967b-4beb-9593-c0aea74b474a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.62544931 1.92602879 1.33561259 0.         1.06108748]\n",
      "\n",
      "[1.62544931 1.92602879 1.33561259 0.         1.06108748]\n"
     ]
    }
   ],
   "source": [
    "print(datasets['EB_test']['HLT_AD_scores'][0:5])\n",
    "print()\n",
    "print(datasets['EB_test']['ONNX_HLT_AD_scores'][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706dfe9-b68b-453f-a6cf-4a21d13c02e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Success! These AD scores are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1518c79-ab47-415a-8bd9-65099d34c6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "powering on... preparing to run evals\n",
      "Loaded A14N23LO from ./h5_ntuples/A14N23LO.h5\n",
      "Loaded EB from ./h5_ntuples/EB.h5\n",
      "Loaded EB_test2 from ./h5_ntuples/EB_test2.h5\n",
      "Loaded EB_train from ./h5_ntuples/EB_train.h5\n",
      "Loaded HAHMggfZdZd2l2nu from ./h5_ntuples/HAHMggfZdZd2l2nu.h5\n",
      "Loaded HHbbttHadHad from ./h5_ntuples/HHbbttHadHad.h5\n",
      "Loaded HLT_noalg_eb_L1All from ./h5_ntuples/HLT_noalg_eb_L1All.h5\n",
      "Loaded ZZ4lep from ./h5_ntuples/ZZ4lep.h5\n",
      "Loaded Zprime2EJs from ./h5_ntuples/Zprime2EJs.h5\n",
      "Loaded jjJZ1 from ./h5_ntuples/jjJZ1.h5\n",
      "Loaded jjJZ2 from ./h5_ntuples/jjJZ2.h5\n",
      "Loaded jjJZ4 from ./h5_ntuples/jjJZ4.h5\n",
      "Loaded qqa from ./h5_ntuples/qqa.h5\n",
      "evals phase 1 of 2 initiated.\n",
      "phase 1: starting evals of model 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-26 07:01:40.362610: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-26 07:01:40.377635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-26 07:01:40.377859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-26 07:01:40.381959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-26 07:01:40.382154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-26 07:01:40.382424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-26 07:01:40.492441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-26 07:01:40.492680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-26 07:01:40.492849: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-26 07:01:40.493001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:07.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLTAD threshold: 9.286888124404394\n",
      "HLTAD threshold: 4.561022975888879\n",
      "phase 1: starting evals of model 1...\n",
      "HLTAD threshold: 8.733034961640545\n",
      "HLTAD threshold: 4.752744620679564\n",
      "phase 1: starting evals of model 2...\n",
      "HLTAD threshold: 5.498442664832609\n",
      "HLTAD threshold: 4.022619764534142\n",
      "phase 1: starting evals of model 3...\n",
      "HLTAD threshold: 8.63785676118642\n",
      "HLTAD threshold: 3.9006244314475182\n",
      "phase 1: starting evals of model 4...\n",
      "HLTAD threshold: 8.7621613195813\n",
      "HLTAD threshold: 3.740239170525538\n",
      "phase 1: starting evals of model 5...\n",
      "HLTAD threshold: 8.733262628992186\n",
      "HLTAD threshold: 3.655159783666013\n",
      "phase 1: starting evals of model 6...\n",
      "HLTAD threshold: 9.330566657464805\n",
      "HLTAD threshold: 4.258327901457669\n",
      "phase 1: starting evals of model 7...\n",
      "HLTAD threshold: 8.839285093274306\n",
      "HLTAD threshold: 3.8142383037773233\n",
      "phase 1: starting evals of model 8...\n",
      "HLTAD threshold: 8.61077566014482\n",
      "HLTAD threshold: 3.8344466905367085\n",
      "phase 1: starting evals of model 9...\n",
      "HLTAD threshold: 8.879100235969752\n",
      "HLTAD threshold: 4.06442334523805\n",
      "evals phase 1 complete.\n",
      "evals phase 2 of 2 initiated.\n",
      "evals phase 2 complete, powering down...\n",
      "goodbye.\n"
     ]
    }
   ],
   "source": [
    "# Rerun evals for trial 1 with correct test data (require test data to have passed L1)\n",
    "\n",
    "data_info = {\n",
    "    \"train_data_scheme\": \"topo2A_train+L1noalg_HLTall\", \n",
    "     \"pt_normalization_type\": \"global_division\", \n",
    "     \"L1AD_rate\": L1AD_rate\n",
    "}\n",
    "\n",
    "training_info = {\n",
    "    \"save_path\": \"./trained_models/multiple_trainings/trial_1\", \n",
    "    \"dropout_p\": 0.1, \n",
    "    \"L2_reg_coupling\": 0.01, \n",
    "    \"latent_dim\": 4, \n",
    "    \"large_network\": True, \n",
    "    \"num_trainings\": 10\n",
    "}\n",
    "\n",
    "ef.process_multiple_models(\n",
    "    training_info=training_info,\n",
    "    data_info=data_info,\n",
    "    plots_path=training_info['save_path']+'_fixed_test_data/plots',\n",
    "    target_rate=target_rate,\n",
    "    L1AD_rate=L1AD_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa68f7c1-7192-451d-8f6b-6b72c6e74eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "powering on... preparing to run evals\n",
      "Loaded A14N23LO from ./h5_ntuples/A14N23LO.h5\n",
      "Loaded EB from ./h5_ntuples/EB.h5\n",
      "Loaded EB_test2 from ./h5_ntuples/EB_test2.h5\n",
      "Loaded EB_train from ./h5_ntuples/EB_train.h5\n",
      "Loaded HAHMggfZdZd2l2nu from ./h5_ntuples/HAHMggfZdZd2l2nu.h5\n",
      "Loaded HHbbttHadHad from ./h5_ntuples/HHbbttHadHad.h5\n",
      "Loaded HLT_noalg_eb_L1All from ./h5_ntuples/HLT_noalg_eb_L1All.h5\n",
      "Loaded ZZ4lep from ./h5_ntuples/ZZ4lep.h5\n",
      "Loaded Zprime2EJs from ./h5_ntuples/Zprime2EJs.h5\n",
      "Loaded jjJZ1 from ./h5_ntuples/jjJZ1.h5\n",
      "Loaded jjJZ2 from ./h5_ntuples/jjJZ2.h5\n",
      "Loaded jjJZ4 from ./h5_ntuples/jjJZ4.h5\n",
      "Loaded qqa from ./h5_ntuples/qqa.h5\n",
      "evals phase 1 of 2 initiated.\n",
      "phase 1: starting evals of model 0...\n",
      "HLTAD threshold: 7.069826961453727\n",
      "HLTAD threshold: 5.324225057374288\n",
      "phase 1: starting evals of model 1...\n",
      "HLTAD threshold: 7.03006376295605\n",
      "HLTAD threshold: 5.06360493238033\n",
      "phase 1: starting evals of model 2...\n",
      "HLTAD threshold: 7.042411270027907\n",
      "HLTAD threshold: 5.1820545487871\n",
      "phase 1: starting evals of model 3...\n",
      "HLTAD threshold: 11.931689654189293\n",
      "HLTAD threshold: 5.331122461090707\n",
      "phase 1: starting evals of model 4...\n",
      "HLTAD threshold: 9.231384877670347\n",
      "HLTAD threshold: 5.377823468602061\n",
      "phase 1: starting evals of model 5...\n",
      "HLTAD threshold: 8.223323170096265\n",
      "HLTAD threshold: 4.657697055267408\n",
      "phase 1: starting evals of model 6...\n",
      "HLTAD threshold: 14.74301902121362\n",
      "HLTAD threshold: 5.026365408879344\n",
      "phase 1: starting evals of model 7...\n",
      "HLTAD threshold: 15.141300599278054\n",
      "HLTAD threshold: 5.1573702965907176\n",
      "phase 1: starting evals of model 8...\n",
      "HLTAD threshold: 6.2152844290172755\n",
      "HLTAD threshold: 4.947721530518063\n",
      "phase 1: starting evals of model 9...\n",
      "HLTAD threshold: 7.811869990822496\n",
      "HLTAD threshold: 4.683644472549695\n",
      "evals phase 1 complete.\n",
      "evals phase 2 of 2 initiated.\n",
      "evals phase 2 complete, powering down...\n",
      "goodbye.\n"
     ]
    }
   ],
   "source": [
    "# Rerun evals for trial 2 with correct test data (require test data to have passed L1)\n",
    "\n",
    "data_info = {\n",
    "    \"train_data_scheme\": \"topo2A_train+overlap\", \n",
    "     \"pt_normalization_type\": \"global_division\", \n",
    "     \"L1AD_rate\": L1AD_rate\n",
    "}\n",
    "\n",
    "training_info = {\n",
    "    \"save_path\": \"./trained_models/multiple_trainings/trial_2\", \n",
    "    \"dropout_p\": 0.1, \n",
    "    \"L2_reg_coupling\": 0.01, \n",
    "    \"latent_dim\": 4, \n",
    "    \"large_network\": True, \n",
    "    \"num_trainings\": 10\n",
    "}\n",
    "\n",
    "ef.process_multiple_models(\n",
    "    training_info=training_info,\n",
    "    data_info=data_info,\n",
    "    plots_path=training_info['save_path']+'_fixed_test_data/plots',\n",
    "    target_rate=target_rate,\n",
    "    L1AD_rate=L1AD_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c064373a-a564-44d1-b4c5-7eb0726b2727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "powering on... preparing to run evals\n",
      "Loaded A14N23LO from ./h5_ntuples/A14N23LO.h5\n",
      "Loaded EB from ./h5_ntuples/EB.h5\n",
      "Loaded EB_test2 from ./h5_ntuples/EB_test2.h5\n",
      "Loaded EB_train from ./h5_ntuples/EB_train.h5\n",
      "Loaded HAHMggfZdZd2l2nu from ./h5_ntuples/HAHMggfZdZd2l2nu.h5\n",
      "Loaded HHbbttHadHad from ./h5_ntuples/HHbbttHadHad.h5\n",
      "Loaded HLT_noalg_eb_L1All from ./h5_ntuples/HLT_noalg_eb_L1All.h5\n",
      "Loaded ZZ4lep from ./h5_ntuples/ZZ4lep.h5\n",
      "Loaded Zprime2EJs from ./h5_ntuples/Zprime2EJs.h5\n",
      "Loaded jjJZ1 from ./h5_ntuples/jjJZ1.h5\n",
      "Loaded jjJZ2 from ./h5_ntuples/jjJZ2.h5\n",
      "Loaded jjJZ4 from ./h5_ntuples/jjJZ4.h5\n",
      "Loaded qqa from ./h5_ntuples/qqa.h5\n",
      "evals phase 1 of 2 initiated.\n",
      "phase 1: starting evals of model 0...\n",
      "HLTAD threshold: 7.656267000375177\n",
      "HLTAD threshold: 3.754804002275115\n",
      "phase 1: starting evals of model 1...\n",
      "HLTAD threshold: 7.700342502939914\n",
      "HLTAD threshold: 3.7171524179343565\n",
      "phase 1: starting evals of model 2...\n",
      "HLTAD threshold: 6.605279562879243\n",
      "HLTAD threshold: 4.038791468866988\n",
      "phase 1: starting evals of model 3...\n",
      "HLTAD threshold: 7.33175794696906\n",
      "HLTAD threshold: 4.166079038481039\n",
      "phase 1: starting evals of model 4...\n",
      "HLTAD threshold: 6.4708440925167245\n",
      "HLTAD threshold: 3.1431100199084034\n",
      "phase 1: starting evals of model 5...\n",
      "HLTAD threshold: 7.078909084975754\n",
      "HLTAD threshold: 5.176855348304076\n",
      "phase 1: starting evals of model 6...\n",
      "HLTAD threshold: 6.535172199495286\n",
      "HLTAD threshold: 3.6050236621176994\n",
      "phase 1: starting evals of model 7...\n",
      "HLTAD threshold: 6.525658287044115\n",
      "HLTAD threshold: 3.2671754942938804\n",
      "phase 1: starting evals of model 8...\n",
      "HLTAD threshold: 8.110608217881323\n",
      "HLTAD threshold: 3.6898088160995\n",
      "phase 1: starting evals of model 9...\n",
      "HLTAD threshold: 6.453700707981839\n",
      "HLTAD threshold: 3.6556608304991776\n",
      "evals phase 1 complete.\n",
      "evals phase 2 of 2 initiated.\n",
      "evals phase 2 complete, powering down...\n",
      "goodbye.\n"
     ]
    }
   ],
   "source": [
    "# Rerun evals for trial 3 with correct test data (require test data to have passed L1)\n",
    "\n",
    "data_info = {\n",
    "    \"train_data_scheme\": \"L1noalg_HLTall\", \n",
    "     \"pt_normalization_type\": \"global_division\", \n",
    "     \"L1AD_rate\": L1AD_rate\n",
    "}\n",
    "\n",
    "training_info = {\n",
    "    \"save_path\": \"./trained_models/multiple_trainings/trial_3\", \n",
    "    \"dropout_p\": 0.1, \n",
    "    \"L2_reg_coupling\": 0.01, \n",
    "    \"latent_dim\": 4, \n",
    "    \"large_network\": True, \n",
    "    \"num_trainings\": 10\n",
    "}\n",
    "\n",
    "ef.process_multiple_models(\n",
    "    training_info=training_info,\n",
    "    data_info=data_info,\n",
    "    plots_path=training_info['save_path']+'_fixed_test_data/plots',\n",
    "    target_rate=target_rate,\n",
    "    L1AD_rate=L1AD_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd2efb-18f3-48de-8b4c-c691340c1cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "powering on... preparing to run evals\n",
      "Loaded A14N23LO from ./h5_ntuples/A14N23LO.h5\n",
      "Loaded EB from ./h5_ntuples/EB.h5\n",
      "Loaded EB_test2 from ./h5_ntuples/EB_test2.h5\n",
      "Loaded EB_train from ./h5_ntuples/EB_train.h5\n",
      "Loaded HAHMggfZdZd2l2nu from ./h5_ntuples/HAHMggfZdZd2l2nu.h5\n",
      "Loaded HHbbttHadHad from ./h5_ntuples/HHbbttHadHad.h5\n",
      "Loaded HLT_noalg_eb_L1All from ./h5_ntuples/HLT_noalg_eb_L1All.h5\n",
      "Loaded ZZ4lep from ./h5_ntuples/ZZ4lep.h5\n",
      "Loaded Zprime2EJs from ./h5_ntuples/Zprime2EJs.h5\n",
      "Loaded jjJZ1 from ./h5_ntuples/jjJZ1.h5\n",
      "Loaded jjJZ2 from ./h5_ntuples/jjJZ2.h5\n",
      "Loaded jjJZ4 from ./h5_ntuples/jjJZ4.h5\n",
      "Loaded qqa from ./h5_ntuples/qqa.h5\n",
      "evals phase 1 of 2 initiated.\n",
      "phase 1: starting evals of model 0...\n",
      "HLTAD threshold: 1.4599743448750353\n",
      "HLTAD threshold: 2.028436779010568\n",
      "phase 1: starting evals of model 1...\n",
      "HLTAD threshold: 1.4986319312620693\n",
      "HLTAD threshold: 2.0434982870312806\n",
      "phase 1: starting evals of model 2...\n",
      "HLTAD threshold: 1.5118034763737196\n",
      "HLTAD threshold: 2.2659821200397765\n",
      "phase 1: starting evals of model 3...\n",
      "HLTAD threshold: 1.5075087426003098\n",
      "HLTAD threshold: 1.9359884641066805\n",
      "phase 1: starting evals of model 4...\n",
      "HLTAD threshold: 1.5044281928996175\n",
      "HLTAD threshold: 2.247638876240912\n",
      "phase 1: starting evals of model 5...\n",
      "HLTAD threshold: 1.4742090312039724\n",
      "HLTAD threshold: 2.0491889287290657\n",
      "phase 1: starting evals of model 6...\n",
      "HLTAD threshold: 1.477636155409413\n",
      "HLTAD threshold: 2.032906816378116\n",
      "phase 1: starting evals of model 7...\n",
      "HLTAD threshold: 1.5098689826992235\n",
      "HLTAD threshold: 2.1182916190417216\n",
      "phase 1: starting evals of model 8...\n"
     ]
    }
   ],
   "source": [
    "# Rerun evals for trial 4 with correct test data (require test data to have passed L1)\n",
    "\n",
    "data_info = {\n",
    "    \"train_data_scheme\": \"topo2A_train+overlap\", \n",
    "     \"pt_normalization_type\": \"StandardScaler\", \n",
    "     \"L1AD_rate\": L1AD_rate\n",
    "}\n",
    "\n",
    "training_info = {\n",
    "    \"save_path\": \"./trained_models/multiple_trainings/trial_4\", \n",
    "    \"dropout_p\": 0.1, \n",
    "    \"L2_reg_coupling\": 0.01, \n",
    "    \"latent_dim\": 4, \n",
    "    \"large_network\": True, \n",
    "    \"num_trainings\": 10\n",
    "}\n",
    "\n",
    "ef.process_multiple_models(\n",
    "    training_info=training_info,\n",
    "    data_info=data_info,\n",
    "    plots_path=training_info['save_path']+'_fixed_test_data/plots',\n",
    "    target_rate=target_rate,\n",
    "    L1AD_rate=L1AD_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9983e8-6e74-48de-bd2a-521107ef43ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun evals for trial 5 with correct test data (require test data to have passed L1)\n",
    "\n",
    "data_info = {\n",
    "    \"train_data_scheme\": \"topo2A_train+overlap\", \n",
    "     \"pt_normalization_type\": \"per_event\", \n",
    "     \"L1AD_rate\": L1AD_rate\n",
    "}\n",
    "\n",
    "training_info = {\n",
    "    \"save_path\": \"./trained_models/multiple_trainings/trial_5\", \n",
    "    \"dropout_p\": 0.1, \n",
    "    \"L2_reg_coupling\": 0.01, \n",
    "    \"latent_dim\": 4, \n",
    "    \"large_network\": True, \n",
    "    \"num_trainings\": 10\n",
    "}\n",
    "\n",
    "ef.process_multiple_models(\n",
    "    training_info=training_info,\n",
    "    data_info=data_info,\n",
    "    plots_path=training_info['save_path']+'_fixed_test_data/plots',\n",
    "    target_rate=target_rate,\n",
    "    L1AD_rate=L1AD_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4b3dbc-c532-43c1-ae8d-e037bcf0b5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1447d7-aff1-47e0-9f66-c3fd6dd1b6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bcbd681-0e4a-4ec1-ab53-f7bd290d5e9c",
   "metadata": {},
   "source": [
    "### Main scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc193906-ab9c-44e9-99ed-563de39ea3b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded A14N23LO from ./h5_ntuples/A14N23LO.h5\n",
      "Loaded EB from ./h5_ntuples/EB.h5\n",
      "Loaded EB_test2 from ./h5_ntuples/EB_test2.h5\n",
      "Loaded EB_train from ./h5_ntuples/EB_train.h5\n",
      "Loaded HAHMggfZdZd2l2nu from ./h5_ntuples/HAHMggfZdZd2l2nu.h5\n",
      "Loaded HHbbttHadHad from ./h5_ntuples/HHbbttHadHad.h5\n",
      "Loaded HLT_noalg_eb_L1All from ./h5_ntuples/HLT_noalg_eb_L1All.h5\n",
      "Loaded ZZ4lep from ./h5_ntuples/ZZ4lep.h5\n",
      "Loaded Zprime2EJs from ./h5_ntuples/Zprime2EJs.h5\n",
      "Loaded jjJZ1 from ./h5_ntuples/jjJZ1.h5\n",
      "Loaded jjJZ2 from ./h5_ntuples/jjJZ2.h5\n",
      "Loaded jjJZ4 from ./h5_ntuples/jjJZ4.h5\n",
      "Loaded qqa from ./h5_ntuples/qqa.h5\n",
      "Booting up... initializing trainings of 10 models\n",
      "\n",
      "starting training model 0...\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "model 0 success\n",
      "\n",
      "starting training model 1...\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "model 1 success\n",
      "\n",
      "starting training model 2...\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "model 2 success\n",
      "\n",
      "starting training model 3...\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "model 3 success\n",
      "\n",
      "starting training model 4...\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "model 4 success\n",
      "\n",
      "starting training model 5...\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "model 5 success\n",
      "\n",
      "starting training model 6...\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "model 6 success\n",
      "\n",
      "starting training model 7...\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "model 7 success\n",
      "\n",
      "starting training model 8...\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "model 8 success\n",
      "\n",
      "starting training model 9...\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "model 9 success\n",
      "\n",
      "Powering off... finished trainings.\n"
     ]
    }
   ],
   "source": [
    "data_info = {\n",
    "    \"train_data_scheme\": \"topo2A_train+overlap\", \n",
    "    \"pt_normalization_type\": \"StandardScaler\", \n",
    "    \"L1AD_rate\": 1000\n",
    "}\n",
    "\n",
    "training_info = {\n",
    "    \"save_path\": \"./trained_models/multiple_trainings/trial_8\", \n",
    "    \"dropout_p\": 0.1, \n",
    "    \"L2_reg_coupling\": 0.01, \n",
    "    \"latent_dim\": 4, \n",
    "    \"large_network\": True, \n",
    "    \"num_trainings\": 10,\n",
    "    \"training_weights\": True\n",
    "}\n",
    "\n",
    "datasets, data_info = ef.load_and_preprocess(**data_info)\n",
    "training_info, data_info = ef.train_multiple_models(datasets, data_info, **training_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fba24f4e-09dd-4843-87b2-83f2012966a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "powering on... preparing to run evals\n",
      "Loaded A14N23LO from ./h5_ntuples/A14N23LO.h5\n",
      "Loaded EB from ./h5_ntuples/EB.h5\n",
      "Loaded EB_test2 from ./h5_ntuples/EB_test2.h5\n",
      "Loaded EB_train from ./h5_ntuples/EB_train.h5\n",
      "Loaded HAHMggfZdZd2l2nu from ./h5_ntuples/HAHMggfZdZd2l2nu.h5\n",
      "Loaded HHbbttHadHad from ./h5_ntuples/HHbbttHadHad.h5\n",
      "Loaded HLT_noalg_eb_L1All from ./h5_ntuples/HLT_noalg_eb_L1All.h5\n",
      "Loaded ZZ4lep from ./h5_ntuples/ZZ4lep.h5\n",
      "Loaded Zprime2EJs from ./h5_ntuples/Zprime2EJs.h5\n",
      "Loaded jjJZ1 from ./h5_ntuples/jjJZ1.h5\n",
      "Loaded jjJZ2 from ./h5_ntuples/jjJZ2.h5\n",
      "Loaded jjJZ4 from ./h5_ntuples/jjJZ4.h5\n",
      "Loaded qqa from ./h5_ntuples/qqa.h5\n",
      "evals phase 1 of 2 initiated.\n",
      "phase 1: starting evals of model 0...\n",
      "HLTAD threshold: 29.844683214341877\n",
      "HLTAD threshold: 34.37642446134512\n",
      "phase 1: starting evals of model 1...\n",
      "HLTAD threshold: 29.84450039083802\n",
      "HLTAD threshold: 34.247196032071706\n",
      "phase 1: starting evals of model 2...\n",
      "HLTAD threshold: 27.04230594704984\n",
      "HLTAD threshold: 34.2360536488412\n",
      "phase 1: starting evals of model 3...\n",
      "HLTAD threshold: 29.84456345791554\n",
      "HLTAD threshold: 34.308713991745904\n",
      "phase 1: starting evals of model 4...\n",
      "HLTAD threshold: 26.896147876545974\n",
      "HLTAD threshold: 34.255756442282006\n",
      "phase 1: starting evals of model 5...\n",
      "HLTAD threshold: 26.992068160604223\n",
      "HLTAD threshold: 38.81035823120269\n",
      "phase 1: starting evals of model 6...\n",
      "HLTAD threshold: 29.84360861099877\n",
      "HLTAD threshold: 34.279083493686784\n",
      "phase 1: starting evals of model 7...\n",
      "HLTAD threshold: 26.9184224011224\n",
      "HLTAD threshold: 34.240623264600494\n",
      "phase 1: starting evals of model 8...\n",
      "HLTAD threshold: 26.929154806147963\n",
      "HLTAD threshold: 34.260260323194096\n",
      "phase 1: starting evals of model 9...\n",
      "HLTAD threshold: 26.95221191136711\n",
      "HLTAD threshold: 34.25482955617218\n",
      "evals phase 1 complete.\n",
      "evals phase 2 of 2 initiated.\n",
      "evals phase 2 complete, powering down...\n",
      "goodbye.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Info in <TEfficiency::TEfficiency>: given histograms are filled with weights\n",
      "Info in <TROOT::TEfficiency::SetUseWeightedEvents>: Handle weighted events for computing efficiency\n",
      "Warning in <TEfficiency::GetEfficiencyErrorLow>: frequentist confidence intervals for weights are only supported by the normal approximation\n",
      "Info in <TEfficiency::GetEfficiencyErrorLow>: setting statistic option to kFNormal\n",
      "Info in <TCanvas::Print>: png file ./trained_models/multiple_trainings/trial_8/plots/HLT_0_pileup_efficiency.png has been created\n",
      "Info in <TEfficiency::TEfficiency>: given histograms are filled with weights\n",
      "Info in <TROOT::TEfficiency::SetUseWeightedEvents>: Handle weighted events for computing efficiency\n",
      "Warning in <TEfficiency::GetEfficiencyErrorLow>: frequentist confidence intervals for weights are only supported by the normal approximation\n",
      "Info in <TEfficiency::GetEfficiencyErrorLow>: setting statistic option to kFNormal\n",
      "Info in <TCanvas::Print>: png file ./trained_models/multiple_trainings/trial_8/plots/L1_0_pileup_efficiency.png has been created\n",
      "Info in <TEfficiency::TEfficiency>: given histograms are filled with weights\n",
      "Info in <TROOT::TEfficiency::SetUseWeightedEvents>: Handle weighted events for computing efficiency\n",
      "Warning in <TEfficiency::GetEfficiencyErrorLow>: frequentist confidence intervals for weights are only supported by the normal approximation\n",
      "Info in <TEfficiency::GetEfficiencyErrorLow>: setting statistic option to kFNormal\n",
      "Info in <TCanvas::Print>: png file ./trained_models/multiple_trainings/trial_8/plots/HLT_1_pileup_efficiency.png has been created\n",
      "Info in <TEfficiency::TEfficiency>: given histograms are filled with weights\n",
      "Info in <TROOT::TEfficiency::SetUseWeightedEvents>: Handle weighted events for computing efficiency\n",
      "Warning in <TEfficiency::GetEfficiencyErrorLow>: frequentist confidence intervals for weights are only supported by the normal approximation\n",
      "Info in <TEfficiency::GetEfficiencyErrorLow>: setting statistic option to kFNormal\n",
      "Info in <TCanvas::Print>: png file ./trained_models/multiple_trainings/trial_8/plots/L1_1_pileup_efficiency.png has been created\n",
      "Info in <TEfficiency::TEfficiency>: given histograms are filled with weights\n",
      "Info in <TROOT::TEfficiency::SetUseWeightedEvents>: Handle weighted events for computing efficiency\n",
      "Warning in <TEfficiency::GetEfficiencyErrorLow>: frequentist confidence intervals for weights are only supported by the normal approximation\n",
      "Info in <TEfficiency::GetEfficiencyErrorLow>: setting statistic option to kFNormal\n",
      "Info in <TCanvas::Print>: png file ./trained_models/multiple_trainings/trial_8/plots/HLT_2_pileup_efficiency.png has been created\n",
      "Info in <TEfficiency::TEfficiency>: given histograms are filled with weights\n",
      "Info in <TROOT::TEfficiency::SetUseWeightedEvents>: Handle weighted events for computing efficiency\n",
      "Warning in <TEfficiency::GetEfficiencyErrorLow>: frequentist confidence intervals for weights are only supported by the normal approximation\n",
      "Info in <TEfficiency::GetEfficiencyErrorLow>: setting statistic option to kFNormal\n",
      "Info in <TCanvas::Print>: png file ./trained_models/multiple_trainings/trial_8/plots/L1_2_pileup_efficiency.png has been created\n",
      "Info in <TEfficiency::TEfficiency>: given histograms are filled with weights\n",
      "Info in <TROOT::TEfficiency::SetUseWeightedEvents>: Handle weighted events for computing efficiency\n",
      "Warning in <TEfficiency::GetEfficiencyErrorLow>: frequentist confidence intervals for weights are only supported by the normal approximation\n",
      "Info in <TEfficiency::GetEfficiencyErrorLow>: setting statistic option to kFNormal\n",
      "Info in <TCanvas::Print>: png file ./trained_models/multiple_trainings/trial_8/plots/HLT_3_pileup_efficiency.png has been created\n",
      "Info in <TEfficiency::TEfficiency>: given histograms are filled with weights\n",
      "Info in <TROOT::TEfficiency::SetUseWeightedEvents>: Handle weighted events for computing efficiency\n",
      "Warning in <TEfficiency::GetEfficiencyErrorLow>: frequentist confidence intervals for weights are only supported by the normal approximation\n",
      "Info in <TEfficiency::GetEfficiencyErrorLow>: setting statistic option to kFNormal\n",
      "Info in <TCanvas::Print>: png file ./trained_models/multiple_trainings/trial_8/plots/L1_3_pileup_efficiency.png has been created\n",
      "Info in <TEfficiency::TEfficiency>: given histograms are filled with weights\n",
      "Info in <TROOT::TEfficiency::SetUseWeightedEvents>: Handle weighted events for computing efficiency\n",
      "Warning in <TEfficiency::GetEfficiencyErrorLow>: frequentist confidence intervals for weights are only supported by the normal approximation\n",
      "Info in <TEfficiency::GetEfficiencyErrorLow>: setting statistic option to kFNormal\n",
      "Info in <TCanvas::Print>: png file ./trained_models/multiple_trainings/trial_8/plots/HLT_4_pileup_efficiency.png has been created\n",
      "Info in <TEfficiency::TEfficiency>: given histograms are filled with weights\n",
      "Info in <TROOT::TEfficiency::SetUseWeightedEvents>: Handle weighted events for computing efficiency\n",
      "Warning in <TEfficiency::GetEfficiencyErrorLow>: frequentist confidence intervals for weights are only supported by the normal approximation\n",
      "Info in <TEfficiency::GetEfficiencyErrorLow>: setting statistic option to kFNormal\n",
      "Info in <TCanvas::Print>: png file ./trained_models/multiple_trainings/trial_8/plots/L1_4_pileup_efficiency.png has been created\n",
      "Info in <TEfficiency::TEfficiency>: given histograms are filled with weights\n",
      "Info in <TROOT::TEfficiency::SetUseWeightedEvents>: Handle weighted events for computing efficiency\n",
      "Warning in <TEfficiency::GetEfficiencyErrorLow>: frequentist confidence intervals for weights are only supported by the normal approximation\n",
      "Info in <TEfficiency::GetEfficiencyErrorLow>: setting statistic option to kFNormal\n",
      "Info in <TCanvas::Print>: png file ./trained_models/multiple_trainings/trial_8/plots/HLT_5_pileup_efficiency.png has been created\n",
      "Info in <TEfficiency::TEfficiency>: given histograms are filled with weights\n",
      "Info in <TROOT::TEfficiency::SetUseWeightedEvents>: Handle weighted events for computing efficiency\n",
      "Warning in <TEfficiency::GetEfficiencyErrorLow>: frequentist confidence intervals for weights are only supported by the normal approximation\n",
      "Info in <TEfficiency::GetEfficiencyErrorLow>: setting statistic option to kFNormal\n",
      "Info in <TCanvas::Print>: png file ./trained_models/multiple_trainings/trial_8/plots/L1_5_pileup_efficiency.png has been created\n",
      "Info in <TEfficiency::TEfficiency>: given histograms are filled with weights\n",
      "Info in <TROOT::TEfficiency::SetUseWeightedEvents>: Handle weighted events for computing efficiency\n",
      "Warning in <TEfficiency::GetEfficiencyErrorLow>: frequentist confidence intervals for weights are only supported by the normal approximation\n",
      "Info in <TEfficiency::GetEfficiencyErrorLow>: setting statistic option to kFNormal\n",
      "Info in <TCanvas::Print>: png file ./trained_models/multiple_trainings/trial_8/plots/HLT_6_pileup_efficiency.png has been created\n",
      "Info in <TEfficiency::TEfficiency>: given histograms are filled with weights\n",
      "Info in <TROOT::TEfficiency::SetUseWeightedEvents>: Handle weighted events for computing efficiency\n",
      "Warning in <TEfficiency::GetEfficiencyErrorLow>: frequentist confidence intervals for weights are only supported by the normal approximation\n",
      "Info in <TEfficiency::GetEfficiencyErrorLow>: setting statistic option to kFNormal\n",
      "Info in <TCanvas::Print>: png file ./trained_models/multiple_trainings/trial_8/plots/L1_6_pileup_efficiency.png has been created\n",
      "Info in <TEfficiency::TEfficiency>: given histograms are filled with weights\n",
      "Info in <TROOT::TEfficiency::SetUseWeightedEvents>: Handle weighted events for computing efficiency\n",
      "Warning in <TEfficiency::GetEfficiencyErrorLow>: frequentist confidence intervals for weights are only supported by the normal approximation\n",
      "Info in <TEfficiency::GetEfficiencyErrorLow>: setting statistic option to kFNormal\n",
      "Info in <TCanvas::Print>: png file ./trained_models/multiple_trainings/trial_8/plots/HLT_7_pileup_efficiency.png has been created\n",
      "Info in <TEfficiency::TEfficiency>: given histograms are filled with weights\n",
      "Info in <TROOT::TEfficiency::SetUseWeightedEvents>: Handle weighted events for computing efficiency\n",
      "Warning in <TEfficiency::GetEfficiencyErrorLow>: frequentist confidence intervals for weights are only supported by the normal approximation\n",
      "Info in <TEfficiency::GetEfficiencyErrorLow>: setting statistic option to kFNormal\n",
      "Info in <TCanvas::Print>: png file ./trained_models/multiple_trainings/trial_8/plots/L1_7_pileup_efficiency.png has been created\n",
      "Info in <TEfficiency::TEfficiency>: given histograms are filled with weights\n",
      "Info in <TROOT::TEfficiency::SetUseWeightedEvents>: Handle weighted events for computing efficiency\n",
      "Warning in <TEfficiency::GetEfficiencyErrorLow>: frequentist confidence intervals for weights are only supported by the normal approximation\n",
      "Info in <TEfficiency::GetEfficiencyErrorLow>: setting statistic option to kFNormal\n",
      "Info in <TCanvas::Print>: png file ./trained_models/multiple_trainings/trial_8/plots/HLT_8_pileup_efficiency.png has been created\n",
      "Info in <TEfficiency::TEfficiency>: given histograms are filled with weights\n",
      "Info in <TROOT::TEfficiency::SetUseWeightedEvents>: Handle weighted events for computing efficiency\n",
      "Warning in <TEfficiency::GetEfficiencyErrorLow>: frequentist confidence intervals for weights are only supported by the normal approximation\n",
      "Info in <TEfficiency::GetEfficiencyErrorLow>: setting statistic option to kFNormal\n",
      "Info in <TCanvas::Print>: png file ./trained_models/multiple_trainings/trial_8/plots/L1_8_pileup_efficiency.png has been created\n",
      "Info in <TEfficiency::TEfficiency>: given histograms are filled with weights\n",
      "Info in <TROOT::TEfficiency::SetUseWeightedEvents>: Handle weighted events for computing efficiency\n",
      "Warning in <TEfficiency::GetEfficiencyErrorLow>: frequentist confidence intervals for weights are only supported by the normal approximation\n",
      "Info in <TEfficiency::GetEfficiencyErrorLow>: setting statistic option to kFNormal\n",
      "Info in <TCanvas::Print>: png file ./trained_models/multiple_trainings/trial_8/plots/HLT_9_pileup_efficiency.png has been created\n",
      "Info in <TEfficiency::TEfficiency>: given histograms are filled with weights\n",
      "Info in <TROOT::TEfficiency::SetUseWeightedEvents>: Handle weighted events for computing efficiency\n",
      "Warning in <TEfficiency::GetEfficiencyErrorLow>: frequentist confidence intervals for weights are only supported by the normal approximation\n",
      "Info in <TEfficiency::GetEfficiencyErrorLow>: setting statistic option to kFNormal\n",
      "Info in <TCanvas::Print>: png file ./trained_models/multiple_trainings/trial_8/plots/L1_9_pileup_efficiency.png has been created\n"
     ]
    }
   ],
   "source": [
    "ef.process_multiple_models(\n",
    "    training_info=training_info,\n",
    "    data_info=data_info,\n",
    "    plots_path=training_info['save_path']+'/plots',\n",
    "    target_rate=target_rate,\n",
    "    L1AD_rate=L1AD_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0c01b0-9213-4b5d-915a-93b00f9c6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ef.process_multiple_models(\n",
    "#     training_info=training_info,\n",
    "#     data_info=data_info,\n",
    "#     plots_path=training_info['save_path']+'/plots',\n",
    "#     target_rate=target_rate,\n",
    "#     L1AD_rate=L1AD_rate\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97e3fa2-6f98-49de-84a5-f72efaac9f4b",
   "metadata": {},
   "source": [
    "Changes to make:\n",
    "- different L1AD rates\n",
    "- l1All ROC\n",
    "- l1All efficiency distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda6be3d-803f-46ed-8d4d-4feac19643f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### save data with AD scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf0772f1-e3fb-4230-bd7c-32f7f6526037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded A14N23LO from ./h5_ntuples/A14N23LO.h5\n",
      "Loaded EB from ./h5_ntuples/EB.h5\n",
      "Loaded EB_test2 from ./h5_ntuples/EB_test2.h5\n",
      "Loaded EB_train from ./h5_ntuples/EB_train.h5\n",
      "Loaded HAHMggfZdZd2l2nu from ./h5_ntuples/HAHMggfZdZd2l2nu.h5\n",
      "Loaded HHbbttHadHad from ./h5_ntuples/HHbbttHadHad.h5\n",
      "Loaded HLT_noalg_eb_L1All from ./h5_ntuples/HLT_noalg_eb_L1All.h5\n",
      "Loaded ZZ4lep from ./h5_ntuples/ZZ4lep.h5\n",
      "Loaded Zprime2EJs from ./h5_ntuples/Zprime2EJs.h5\n",
      "Loaded jjJZ1 from ./h5_ntuples/jjJZ1.h5\n",
      "Loaded jjJZ2 from ./h5_ntuples/jjJZ2.h5\n",
      "Loaded jjJZ4 from ./h5_ntuples/jjJZ4.h5\n",
      "Loaded qqa from ./h5_ntuples/qqa.h5\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "2188/2188 [==============================] - 3s 1ms/step\n",
      "2188/2188 [==============================] - 3s 1ms/step\n",
      "3125/3125 [==============================] - 5s 2ms/step\n",
      "3125/3125 [==============================] - 5s 2ms/step\n",
      "3125/3125 [==============================] - 5s 2ms/step\n",
      "3125/3125 [==============================] - 5s 1ms/step\n",
      "3125/3125 [==============================] - 5s 2ms/step\n",
      "3125/3125 [==============================] - 5s 2ms/step\n",
      "3125/3125 [==============================] - 5s 2ms/step\n",
      "3125/3125 [==============================] - 5s 2ms/step\n",
      "3125/3125 [==============================] - 5s 2ms/step\n",
      "3125/3125 [==============================] - 5s 2ms/step\n",
      "3125/3125 [==============================] - 5s 2ms/step\n",
      "3125/3125 [==============================] - 5s 1ms/step\n",
      "1563/1563 [==============================] - 2s 2ms/step\n",
      "1563/1563 [==============================] - 2s 1ms/step\n",
      "15912/15912 [==============================] - 24s 2ms/step\n",
      "15912/15912 [==============================] - 24s 1ms/step\n",
      "Saved A14N23LO to /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/A14N23LO.h5\n",
      "Saved HAHMggfZdZd2l2nu to /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/HAHMggfZdZd2l2nu.h5\n",
      "Saved HHbbttHadHad to /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/HHbbttHadHad.h5\n",
      "Saved ZZ4lep to /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/ZZ4lep.h5\n",
      "Saved Zprime2EJs to /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/Zprime2EJs.h5\n",
      "Saved jjJZ1 to /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/jjJZ1.h5\n",
      "Saved jjJZ2 to /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/jjJZ2.h5\n",
      "Saved jjJZ4 to /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/jjJZ4.h5\n",
      "Saved qqa to /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/qqa.h5\n",
      "Saved EB_test to /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/EB_test.h5\n",
      "Saved EB_train to /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/EB_train.h5\n",
      "Saved EB_val to /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/EB_val.h5\n"
     ]
    }
   ],
   "source": [
    "# Save version with AD scores\n",
    "training_info = {\n",
    "    \"save_path\": \"./trained_models/multiple_trainings/trial_2\", \n",
    "    \"dropout_p\": 0.1, \n",
    "    \"L2_reg_coupling\": 0.01, \n",
    "    \"latent_dim\": 4, \n",
    "    \"large_network\": True, \n",
    "    \"num_trainings\": 10,\n",
    "    \"training_weights\": True\n",
    "}\n",
    "\n",
    "data_info = {\n",
    "    \"train_data_scheme\": \n",
    "    \"topo2A_train+overlap\", \n",
    "    \"pt_normalization_type\": \n",
    "    \"global_division\", \n",
    "    \"L1AD_rate\": 1000\n",
    "}\n",
    "\n",
    "datasets, data_info = ef.load_and_preprocess(**data_info)\n",
    "ef.save_datasets_with_AD_scores(datasets, training_info=training_info, save_dir='/eos/home-m/mmcohen/ntuples/dataset_with_AD_scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b82a76-49f2-4506-8c02-9100b700a862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded A14N23LO from /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/A14N23LO.h5\n",
      "Loaded EB_test from /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/EB_test.h5\n",
      "Loaded EB_train from /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/EB_train.h5\n",
      "Loaded EB_val from /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/EB_val.h5\n",
      "Loaded HAHMggfZdZd2l2nu from /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/HAHMggfZdZd2l2nu.h5\n",
      "Loaded HHbbttHadHad from /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/HHbbttHadHad.h5\n",
      "Loaded ZZ4lep from /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/ZZ4lep.h5\n",
      "Loaded Zprime2EJs from /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/Zprime2EJs.h5\n",
      "Loaded jjJZ1 from /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/jjJZ1.h5\n",
      "Loaded jjJZ2 from /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/jjJZ2.h5\n",
      "Loaded jjJZ4 from /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/jjJZ4.h5\n",
      "Loaded qqa from /eos/home-m/mmcohen/ntuples/dataset_with_AD_scores/qqa.h5\n",
      "A14N23LO:\n",
      "    HLT_AD_scores: (10000,)\n",
      "    HLT_data: (10000, 48)\n",
      "    HLT_latent_reps: (10000, 4)\n",
      "    HLT_model_outputs: (10000, 48)\n",
      "    L1Seeded: (10000,)\n",
      "    L1_AD_scores: (10000,)\n",
      "    L1_data: (10000, 48)\n",
      "    L1_latent_reps: (10000, 4)\n",
      "    L1_model_outputs: (10000, 48)\n",
      "    passHLT: (10000,)\n",
      "    passL1: (10000,)\n",
      "    raw_HLT_pt: (10000, 16)\n",
      "    raw_L1_pt: (10000, 16)\n",
      "    topo2A_AD_scores: (10000,)\n",
      "    weights: (10000,)\n",
      "EB_test:\n",
      "    HLT_AD_scores: (509167,)\n",
      "    HLT_data: (509167, 48)\n",
      "    HLT_latent_reps: (509167, 4)\n",
      "    HLT_model_outputs: (509167, 48)\n",
      "    L1Seeded: (509167,)\n",
      "    L1_AD_scores: (509167,)\n",
      "    L1_data: (509167, 48)\n",
      "    L1_latent_reps: (509167, 4)\n",
      "    L1_model_outputs: (509167, 48)\n",
      "    event_numbers: (509167,)\n",
      "    passHLT: (509167,)\n",
      "    passL1: (509167,)\n",
      "    pileups: (509167,)\n",
      "    raw_HLT_pt: (509167, 16)\n",
      "    raw_L1_pt: (509167, 16)\n",
      "    run_numbers: (509167,)\n",
      "    topo2A_AD_scores: (509167,)\n",
      "    weights: (509167,)\n",
      "EB_train:\n",
      "    HLT_data: (1493152, 48)\n",
      "    L1Seeded: (1493152,)\n",
      "    L1_data: (1493152, 48)\n",
      "    event_numbers: (1493152,)\n",
      "    passHLT: (1493152,)\n",
      "    passL1: (1493152,)\n",
      "    pileups: (1493152,)\n",
      "    raw_HLT_pt: (1493152, 16)\n",
      "    raw_L1_pt: (1493152, 16)\n",
      "    run_numbers: (1493152,)\n",
      "    topo2A_AD_scores: (1493152,)\n",
      "    weights: (1493152,)\n",
      "EB_val:\n",
      "    HLT_data: (263498, 48)\n",
      "    L1Seeded: (263498,)\n",
      "    L1_data: (263498, 48)\n",
      "    event_numbers: (263498,)\n",
      "    passHLT: (263498,)\n",
      "    passL1: (263498,)\n",
      "    pileups: (263498,)\n",
      "    raw_HLT_pt: (263498, 16)\n",
      "    raw_L1_pt: (263498, 16)\n",
      "    run_numbers: (263498,)\n",
      "    topo2A_AD_scores: (263498,)\n",
      "    weights: (263498,)\n",
      "HAHMggfZdZd2l2nu:\n",
      "    HLT_AD_scores: (70000,)\n",
      "    HLT_data: (70000, 48)\n",
      "    HLT_latent_reps: (70000, 4)\n",
      "    HLT_model_outputs: (70000, 48)\n",
      "    L1Seeded: (70000,)\n",
      "    L1_AD_scores: (70000,)\n",
      "    L1_data: (70000, 48)\n",
      "    L1_latent_reps: (70000, 4)\n",
      "    L1_model_outputs: (70000, 48)\n",
      "    passHLT: (70000,)\n",
      "    passL1: (70000,)\n",
      "    raw_HLT_pt: (70000, 16)\n",
      "    raw_L1_pt: (70000, 16)\n",
      "    topo2A_AD_scores: (70000,)\n",
      "    weights: (70000,)\n",
      "HHbbttHadHad:\n",
      "    HLT_AD_scores: (100000,)\n",
      "    HLT_data: (100000, 48)\n",
      "    HLT_latent_reps: (100000, 4)\n",
      "    HLT_model_outputs: (100000, 48)\n",
      "    L1Seeded: (100000,)\n",
      "    L1_AD_scores: (100000,)\n",
      "    L1_data: (100000, 48)\n",
      "    L1_latent_reps: (100000, 4)\n",
      "    L1_model_outputs: (100000, 48)\n",
      "    passHLT: (100000,)\n",
      "    passL1: (100000,)\n",
      "    raw_HLT_pt: (100000, 16)\n",
      "    raw_L1_pt: (100000, 16)\n",
      "    topo2A_AD_scores: (100000,)\n",
      "    weights: (100000,)\n",
      "ZZ4lep:\n",
      "    HLT_AD_scores: (100000,)\n",
      "    HLT_data: (100000, 48)\n",
      "    HLT_latent_reps: (100000, 4)\n",
      "    HLT_model_outputs: (100000, 48)\n",
      "    L1Seeded: (100000,)\n",
      "    L1_AD_scores: (100000,)\n",
      "    L1_data: (100000, 48)\n",
      "    L1_latent_reps: (100000, 4)\n",
      "    L1_model_outputs: (100000, 48)\n",
      "    passHLT: (100000,)\n",
      "    passL1: (100000,)\n",
      "    raw_HLT_pt: (100000, 16)\n",
      "    raw_L1_pt: (100000, 16)\n",
      "    topo2A_AD_scores: (100000,)\n",
      "    weights: (100000,)\n",
      "Zprime2EJs:\n",
      "    HLT_AD_scores: (100000,)\n",
      "    HLT_data: (100000, 48)\n",
      "    HLT_latent_reps: (100000, 4)\n",
      "    HLT_model_outputs: (100000, 48)\n",
      "    L1Seeded: (100000,)\n",
      "    L1_AD_scores: (100000,)\n",
      "    L1_data: (100000, 48)\n",
      "    L1_latent_reps: (100000, 4)\n",
      "    L1_model_outputs: (100000, 48)\n",
      "    passHLT: (100000,)\n",
      "    passL1: (100000,)\n",
      "    raw_HLT_pt: (100000, 16)\n",
      "    raw_L1_pt: (100000, 16)\n",
      "    topo2A_AD_scores: (100000,)\n",
      "    weights: (100000,)\n",
      "jjJZ1:\n",
      "    HLT_AD_scores: (100000,)\n",
      "    HLT_data: (100000, 48)\n",
      "    HLT_latent_reps: (100000, 4)\n",
      "    HLT_model_outputs: (100000, 48)\n",
      "    L1Seeded: (100000,)\n",
      "    L1_AD_scores: (100000,)\n",
      "    L1_data: (100000, 48)\n",
      "    L1_latent_reps: (100000, 4)\n",
      "    L1_model_outputs: (100000, 48)\n",
      "    passHLT: (100000,)\n",
      "    passL1: (100000,)\n",
      "    raw_HLT_pt: (100000, 16)\n",
      "    raw_L1_pt: (100000, 16)\n",
      "    topo2A_AD_scores: (100000,)\n",
      "    weights: (100000,)\n",
      "jjJZ2:\n",
      "    HLT_AD_scores: (100000,)\n",
      "    HLT_data: (100000, 48)\n",
      "    HLT_latent_reps: (100000, 4)\n",
      "    HLT_model_outputs: (100000, 48)\n",
      "    L1Seeded: (100000,)\n",
      "    L1_AD_scores: (100000,)\n",
      "    L1_data: (100000, 48)\n",
      "    L1_latent_reps: (100000, 4)\n",
      "    L1_model_outputs: (100000, 48)\n",
      "    passHLT: (100000,)\n",
      "    passL1: (100000,)\n",
      "    raw_HLT_pt: (100000, 16)\n",
      "    raw_L1_pt: (100000, 16)\n",
      "    topo2A_AD_scores: (100000,)\n",
      "    weights: (100000,)\n",
      "jjJZ4:\n",
      "    HLT_AD_scores: (100000,)\n",
      "    HLT_data: (100000, 48)\n",
      "    HLT_latent_reps: (100000, 4)\n",
      "    HLT_model_outputs: (100000, 48)\n",
      "    L1Seeded: (100000,)\n",
      "    L1_AD_scores: (100000,)\n",
      "    L1_data: (100000, 48)\n",
      "    L1_latent_reps: (100000, 4)\n",
      "    L1_model_outputs: (100000, 48)\n",
      "    passHLT: (100000,)\n",
      "    passL1: (100000,)\n",
      "    raw_HLT_pt: (100000, 16)\n",
      "    raw_L1_pt: (100000, 16)\n",
      "    topo2A_AD_scores: (100000,)\n",
      "    weights: (100000,)\n",
      "qqa:\n",
      "    HLT_AD_scores: (50000,)\n",
      "    HLT_data: (50000, 48)\n",
      "    HLT_latent_reps: (50000, 4)\n",
      "    HLT_model_outputs: (50000, 48)\n",
      "    L1Seeded: (50000,)\n",
      "    L1_AD_scores: (50000,)\n",
      "    L1_data: (50000, 48)\n",
      "    L1_latent_reps: (50000, 4)\n",
      "    L1_model_outputs: (50000, 48)\n",
      "    passHLT: (50000,)\n",
      "    passL1: (50000,)\n",
      "    raw_HLT_pt: (50000, 16)\n",
      "    raw_L1_pt: (50000, 16)\n",
      "    topo2A_AD_scores: (50000,)\n",
      "    weights: (50000,)\n"
     ]
    }
   ],
   "source": [
    "datasets = ef.load_subdicts_from_h5('/eos/home-m/mmcohen/ntuples/dataset_with_AD_scores')\n",
    "\n",
    "for tag, data_dict in datasets.items():\n",
    "    print(f'{tag}:')\n",
    "    for key, value in data_dict.items():\n",
    "        print(f'    {key}: {value.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8511137-fa7c-45cf-b05f-1eeb919f84b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
