{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48fd2f61-a25d-4b57-aa0a-3ad176c048db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 15:57:08.186256: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-10 15:57:08.268256: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import random\n",
    "import sklearn\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pylab\n",
    "from scipy.optimize import curve_fit\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "#import shap\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tarfile\n",
    "from tensorflow.keras.models import load_model\n",
    "from qkeras import QActivation, QDense, QConv2D, QBatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3622534c-95fc-4c07-b17f-bfab98f99320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list that will hold all of the data (data22 and MC)\n",
    "\n",
    "datasets = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9157d58-4120-4d2a-87c5-15d61e0707c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MC data.\n",
    "\n",
    "data_path = '../../../../ntuples/MC_07-17-2024/'\n",
    "\n",
    "for filename in os.listdir(data_path):\n",
    "\n",
    "    if filename.startswith('N') or filename.startswith('.'): continue\n",
    "\n",
    "    dataset_tag = filename.split('_')[0]\n",
    "    \n",
    "    with h5py.File(data_path+filename, 'r') as hf:\n",
    "        HLT_jets = hf['HLT_jets'][:]\n",
    "        L1_jFexSR_jets = hf['L1_jFexSR_jets'][:]\n",
    "        L1_jFexLR_jets = hf['L1_jFexLR_jets'][:]\n",
    "        HLT_electrons = hf['HLT_electrons'][:]\n",
    "        LRT_electrons = hf['LRT_electrons'][:]\n",
    "        L1_egammas = hf['L1_egammas'][:]\n",
    "        HLT_muons = hf['HLT_muons'][:]\n",
    "        LRT_muons = hf['LRT_muons'][:]\n",
    "        L1_muons = hf['L1_muons'][:]\n",
    "        L1_eFex_taus = hf['L1_eFex_taus'][:]\n",
    "        L1_jFex_taus = hf['L1_jFex_taus'][:]\n",
    "        HLT_photons = hf['HLT_photons'][:]\n",
    "        HLT_MET = hf['HLT_MET'][:].reshape(-1, 1, 4)  # Broadcasting MET\n",
    "        L1_MET = hf['L1_MET'][:].reshape(-1, 1, 3)\n",
    "        pass_L1_unprescaled = hf[\"pass_L1_unprescaled\"][:]\n",
    "        pass_HLT_unprescaled = hf[\"pass_HLT_unprescaled\"][:]\n",
    "\n",
    "        HLT_objects = np.concatenate([HLT_jets[:, :6, [0, 2, 3]], HLT_electrons[:, :3, [0, 2, 3]], HLT_muons[:, :3, [0, 2, 3]], HLT_photons[:, :3, [0, 2, 3]], HLT_MET[:, :, [0, 2, 3]]], axis=1)\n",
    "        L1_objects = np.concatenate([L1_jFexSR_jets[:, :6, :], L1_egammas[:, :3, :], L1_muons[:, :3, :], L1_eFex_taus[:, :3, :], L1_MET], axis=1)\n",
    "        \n",
    "        datasets[dataset_tag] = {\n",
    "            'HLT_data': HLT_objects,\n",
    "            'L1_data': L1_objects,\n",
    "            'passL1': pass_L1_unprescaled==1,\n",
    "            'passHLT': pass_HLT_unprescaled==1,\n",
    "            'weights': np.ones(len(HLT_objects)),\n",
    "        }\n",
    "\n",
    "        if len(HLT_objects) > 100000:\n",
    "            datasets[dataset_tag] = {key: value[:100000] for key, value in datasets[dataset_tag].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7715cd41-bb58-481e-8fdb-7e3bd26e89e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect new EB data that will have incorrect ordering.\n",
    "\n",
    "# Define the base path where the h5 files are stored\n",
    "base_path = '/eos/home-m/mmcohen/ntuples/EB_h5_10-06-2024/'\n",
    "\n",
    "# Initialize empty lists to collect arrays from all files\n",
    "HLT_jets_list = []\n",
    "ofl_jets_list = []\n",
    "L1_jFexSR_jets_list = []\n",
    "L1_jFexLR_jets_list = []\n",
    "HLT_electrons_list = []\n",
    "LRT_electrons_list = []\n",
    "ofl_electrons_list = []\n",
    "L1_egammas_list = []\n",
    "HLT_muons_list = []\n",
    "LRT_muons_list = []\n",
    "ofl_muons_list = []\n",
    "L1_muons_list = []\n",
    "L1_eFex_taus_list = []\n",
    "L1_jFex_taus_list = []\n",
    "HLT_photons_list = []\n",
    "ofl_photons_list = []\n",
    "HLT_MET_list = []\n",
    "L1_MET_list = []\n",
    "pass_L1_unprescaled_list = []\n",
    "pass_HLT_unprescaled_list = []\n",
    "EB_weights_list = []\n",
    "event_number_list = []\n",
    "run_number_list = []\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for file_name in os.listdir(base_path):\n",
    "    file_path = os.path.join(base_path, file_name)\n",
    "    \n",
    "    # Open each h5 file and append data to lists\n",
    "    with h5py.File(file_path, 'r') as hf:\n",
    "        HLT_jets = hf['HLT_jets'][:]\n",
    "        ofl_jets = hf['ofl_jets'][:]\n",
    "        L1_jFexSR_jets = hf['L1_jFexSR_jets'][:]\n",
    "        L1_jFexLR_jets = hf['L1_jFexLR_jets'][:]\n",
    "        HLT_electrons = hf['HLT_electrons'][:]\n",
    "        LRT_electrons = hf['LRT_electrons'][:]\n",
    "        ofl_electrons = hf['ofl_electrons'][:]\n",
    "        L1_egammas = hf['L1_egammas'][:]\n",
    "        HLT_muons = hf['HLT_muons'][:]\n",
    "        LRT_muons = hf['LRT_muons'][:]\n",
    "        ofl_muons = hf['ofl_muons'][:]\n",
    "        L1_muons = hf['L1_muons'][:]\n",
    "        L1_eFex_taus = hf['L1_eFex_taus'][:]\n",
    "        L1_jFex_taus = hf['L1_jFex_taus'][:]\n",
    "        HLT_photons = hf['HLT_photons'][:]\n",
    "        ofl_photons = hf['ofl_photons'][:]\n",
    "        HLT_MET = hf['HLT_MET'][:].reshape(-1, 1, 3)  # Broadcasting MET\n",
    "        L1_MET = hf['L1_MET'][:].reshape(-1, 1, 3)\n",
    "        pass_L1_unprescaled = hf[\"pass_L1_unprescaled\"][:]\n",
    "        pass_HLT_unprescaled = hf[\"pass_HLT_unprescaled\"][:]\n",
    "        EB_weights = hf[\"EB_weights\"][:]\n",
    "        event_number = hf[\"event_number\"][:]\n",
    "        run_number = hf[\"run_number\"][:]\n",
    "        mu = hf[\"mu\"][:]\n",
    "\n",
    "    HLT_objects = np.concatenate([HLT_jets[:, :6, [0, 2, 3]], HLT_electrons[:, :3, :], HLT_muons[:, :3, :], HLT_photons[:, :3, :], HLT_MET], axis=1)\n",
    "    L1_objects = np.concatenate([L1_jFexSR_jets[:, :6, :], L1_egammas[:, :3, :], L1_muons[:, :3, :], L1_eFex_taus[:, :3, :], L1_MET], axis=1)\n",
    "    \n",
    "    datasets[file_name.split('_10')[0]] = {\n",
    "        'HLT_data': HLT_objects,\n",
    "        'L1_data': L1_objects,\n",
    "        'passL1': pass_L1_unprescaled==1,\n",
    "        'passHLT': pass_HLT_unprescaled==1,\n",
    "        'weights': EB_weights,\n",
    "        'event_numbers': event_number,\n",
    "        'run_numbers': run_number,\n",
    "        'pileups': mu\n",
    "    }\n",
    "\n",
    "def combine_data(datasets, tags_to_combine, new_tag):\n",
    "\n",
    "    # initialize empty lists for new tag\n",
    "    datasets[new_tag] = {key: [] for key in datasets[tags_to_combine[0]].keys()}\n",
    "\n",
    "    # Loop through old tags and append np arrays to lists\n",
    "    for tag in tags_to_combine:\n",
    "        for key, value in datasets[tag].items():\n",
    "            datasets[new_tag][key].append(value)\n",
    "\n",
    "    # Concatenate lists into single np array\n",
    "    for key, value in datasets[new_tag].items():\n",
    "        datasets[new_tag][key] = np.concatenate(value, axis=0)\n",
    "\n",
    "    # Delete old tags\n",
    "    for tag in tags_to_combine:\n",
    "        del datasets[tag]\n",
    "\n",
    "    # Make sure everything is an np array\n",
    "    for tag, data_dict in datasets.items():\n",
    "        for key, value in data_dict.items():\n",
    "            data_dict[key] = np.array(value)\n",
    "\n",
    "    return datasets\n",
    "\n",
    "datasets = combine_data(datasets, tags_to_combine=['EB_475341_0', 'EB_475341_1'], new_tag='HLT_noalg_eb_L1All')\n",
    "#datasets = combine_data(datasets, tags_to_combine=['EB_473255_0', 'EB_475321_0', 'EB_482596_0'], new_tag='EB')\n",
    "datasets = combine_data(datasets, tags_to_combine=['EB_473255_0', 'EB_475321_0'], new_tag='EB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a4223dd-6c46-4848-a99e-d5c3c10a2f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_duplicates(arr):\n",
    "    _, counts = np.unique(arr, return_counts=True)\n",
    "    return np.any(counts > 1)\n",
    "\n",
    "def load_and_process_normal_data(file_name):\n",
    "    with h5py.File(file_name, 'r') as hf:\n",
    "        nmuon, nLRjet, nSRjet, negamma, netau, njtau = 4, 6, 6, 4, 4, 4\n",
    "\n",
    "        def load_and_scale(dataset, n_objects, scale_factor=10/1024, eta_factor=10/16, phi_factor = 10/8):\n",
    "            data = hf[dataset][:, 0:n_objects, :]\n",
    "            data[:, :, 0] *= scale_factor  # Scale the pT value\n",
    "            data[:, :, 1] *= eta_factor  # Scale the eta value\n",
    "            data[:, :, 2] *= phi_factor  # Scale the phi value\n",
    "            return data.reshape(-1, 3 * n_objects)\n",
    "\n",
    "        L1_jFexSR_jets = load_and_scale('L1_jFexSR_jets', nSRjet)\n",
    "        L1_jFexLR_jets = load_and_scale('L1_jFexLR_jets', nLRjet)\n",
    "        L1_egammas = load_and_scale('L1_egammas', negamma)\n",
    "        L1_muons = load_and_scale('L1_muons', nmuon, scale_factor=10000/64)  # Specific scaling for muons\n",
    "        L1_eFex_taus = load_and_scale('L1_eFex_taus', netau)\n",
    "        L1_jFex_taus = load_and_scale('L1_jFex_taus', njtau)\n",
    "\n",
    "        L1_MET = hf['L1_MET'][:]\n",
    "        L1_MET[:, 0] *= 10/8192\n",
    "        L1_MET[:, 2] *= 10/8\n",
    "\n",
    "        pass_L1_unprescaled = hf[\"pass_L1_unprescaled\"][:]\n",
    "        pass_HLT_unprescaled = hf[\"pass_HLT_unprescaled\"][:]\n",
    "        EB_weights = hf[\"EB_weights\"][:]\n",
    "        event_id_signal = hf['event_number'][:]\n",
    "        run_id_signal = hf['run_number'][:]\n",
    "\n",
    "        if has_duplicates(event_id_signal):\n",
    "            print(\"event index show up more than once!!!\")\n",
    "        else:\n",
    "            print(\"event index looks good :)\")\n",
    "\n",
    "        # Reformat L1_MET\n",
    "        L1_MET_fixed = np.zeros((L1_MET.shape[0], 2))\n",
    "        L1_MET_fixed[:, 0] = L1_MET[:, 0]\n",
    "        L1_MET_fixed[:, 1] = L1_MET[:, 2]\n",
    "        L1_MET = L1_MET_fixed\n",
    "\n",
    "        # Combine arrays into Topo groups\n",
    "        Topo_2A = np.concatenate([L1_jFexSR_jets, L1_eFex_taus, L1_muons, L1_MET], axis=1)\n",
    "        Topo_2B = np.concatenate([L1_jFexSR_jets, L1_egammas, L1_jFex_taus, L1_MET], axis=1)\n",
    "        Topo_3A = np.concatenate([L1_jFexSR_jets, L1_egammas, L1_eFex_taus, L1_MET], axis=1)\n",
    "\n",
    "        # Handle NaN values\n",
    "        def fill_median(array):\n",
    "            for i in range(array.shape[1]):\n",
    "                median_value = np.nanmedian(array[:, i])\n",
    "                array[np.isnan(array[:, i]), i] = 0  # median_value\n",
    "            return array\n",
    "\n",
    "        Topo_2A = fill_median(Topo_2A)\n",
    "        Topo_2B = fill_median(Topo_2B)\n",
    "        Topo_3A = fill_median(Topo_3A)\n",
    "\n",
    "        return Topo_2A, Topo_2B, Topo_3A, pass_L1_unprescaled, pass_HLT_unprescaled, EB_weights, event_id_signal, run_id_signal\n",
    "\n",
    "def load_and_process_anomalous_data(file_name):\n",
    "    with h5py.File(file_name, 'r') as hf:\n",
    "        nmuon, nLRjet, nSRjet, negamma, netau, njtau = 4, 6, 6, 4, 4, 4\n",
    "        print(hf.keys())\n",
    "\n",
    "        def load_and_scale(dataset, n_objects, scale_factor=10/1024, eta_factor=10/16, phi_factor = 10/8):\n",
    "            data = hf[dataset][:, 0:n_objects, :]\n",
    "            data[:, :, 0] *= scale_factor  # Scale the pT value\n",
    "            data[:, :, 1] *= eta_factor  # Scale the eta value\n",
    "            data[:, :, 2] *= phi_factor  # Scale the phi value\n",
    "            return data.reshape(-1, 3 * n_objects)\n",
    "\n",
    "        L1_jFexSR_jets = load_and_scale('L1_jFexSR_jets', nSRjet)\n",
    "        L1_jFexLR_jets = load_and_scale('L1_jFexLR_jets', nLRjet)\n",
    "        L1_egammas = load_and_scale('L1_egammas', negamma)\n",
    "        L1_muons = load_and_scale('L1_muons', nmuon, scale_factor=10000/64)  # Specific scaling for muons\n",
    "        L1_eFex_taus = load_and_scale('L1_eFex_taus', netau)\n",
    "        L1_jFex_taus = load_and_scale('L1_jFex_taus', njtau)\n",
    "\n",
    "        L1_MET = hf['L1_MET'][:]\n",
    "        L1_MET[:, 0] *= 10/8192\n",
    "        L1_MET[:, 2] *= 10/8\n",
    "\n",
    "        pass_L1_unprescaled = hf[\"pass_L1_unprescaled\"][:]\n",
    "\n",
    "        # Reformat L1_MET\n",
    "        L1_MET_fixed = np.zeros((L1_MET.shape[0], 2))\n",
    "        L1_MET_fixed[:, 0] = L1_MET[:, 0]\n",
    "        L1_MET_fixed[:, 1] = L1_MET[:, 2]\n",
    "        L1_MET = L1_MET_fixed\n",
    "\n",
    "        # Combine arrays into Topo groups\n",
    "        Topo_2A = np.concatenate([L1_jFexSR_jets, L1_eFex_taus, L1_muons, L1_MET], axis=1)\n",
    "        Topo_2B = np.concatenate([L1_jFexSR_jets, L1_egammas, L1_jFex_taus, L1_MET], axis=1)\n",
    "        Topo_3A = np.concatenate([L1_jFexSR_jets, L1_egammas, L1_eFex_taus, L1_MET], axis=1)\n",
    "\n",
    "        # Handle NaN values\n",
    "        def fill_median(array):\n",
    "            for i in range(array.shape[1]):\n",
    "                median_value = np.nanmedian(array[:, i])\n",
    "                array[np.isnan(array[:, i]), i] = 0  # median_value\n",
    "            return array\n",
    "\n",
    "        Topo_2A = fill_median(Topo_2A)\n",
    "        Topo_2B = fill_median(Topo_2B)\n",
    "        Topo_3A = fill_median(Topo_3A)\n",
    "\n",
    "        return Topo_2A, Topo_2B, Topo_3A, pass_L1_unprescaled\n",
    "\n",
    "def load_model_from_targz(targz_path, model_name):\n",
    "    with tarfile.open(targz_path, 'r:gz') as tar:\n",
    "        tar.extractall(path='temp_model')\n",
    "    \n",
    "    model_path = os.path.join('temp_model', model_name)\n",
    "    custom_objects = {\n",
    "        'QDense': QDense,\n",
    "        'QActivation': QActivation,\n",
    "        'QBatchNormalization': QBatchNormalization\n",
    "    }\n",
    "    model = load_model(model_path, custom_objects=custom_objects)\n",
    "    \n",
    "    # Clean up the temporary directory\n",
    "    for root, dirs, files in os.walk('temp_model', topdown=False):\n",
    "        for name in files:\n",
    "            os.remove(os.path.join(root, name))\n",
    "        for name in dirs:\n",
    "            os.rmdir(os.path.join(root, name))\n",
    "    os.rmdir('temp_model')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "505a9788-cb7e-4d80-b602-831baf37677b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event index show up more than once!!!\n",
      "Shape of Topo_2A_test: (350000, 44)\n",
      "Shape of Topo_2A_train: (1754833, 44)\n"
     ]
    }
   ],
   "source": [
    "# Load and process normal data\n",
    "Topo_2A, Topo_2B, Topo_3A, pass_L1_unprescaled, pass_HLT_unprescaled, EB_weights, event_id, run_id = load_and_process_normal_data('/eos/home-m/mmcohen/ntuples/EB_ntuples_08-13-2024.h5')\n",
    "\n",
    "\n",
    "# Splitting training and testing datasets for Topo_2A and weights\n",
    "Topo_2A_train = np.concatenate((Topo_2A[0:450000], Topo_2A[800000:]), axis=0)\n",
    "Topo_train_weights = np.concatenate((EB_weights[0:450000], EB_weights[800000:]), axis=0)\n",
    "train_event_id = np.concatenate((event_id[0:450000], event_id[800000:]), axis=0)\n",
    "train_run_id = np.concatenate((run_id[0:450000], run_id[800000:]), axis=0)\n",
    "\n",
    "Topo_2A_test = Topo_2A[450000:800000, :]\n",
    "test_event_id = event_id[450000:800000]\n",
    "test_run_id = run_id[450000:800000]  # Adding split for run_id in the test set\n",
    "Topo_test_weights = EB_weights[450000:800000]\n",
    "\n",
    "# Output the shapes of the relevant arrays\n",
    "print(\"Shape of Topo_2A_test:\", Topo_2A_test.shape)\n",
    "print(\"Shape of Topo_2A_train:\", Topo_2A_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e5ba220-0f21-41b9-a3ba-9087cf53d6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted shapes:\n",
      "Topo_2A_train: (1754833, 44)\n",
      "Topo_2A_test: (350000, 44)\n",
      "\n",
      "First few elements of train set:\n",
      "Event IDs: [480857418 480857460 480857610 480857642 480857751]\n",
      "Run IDs: [473255 473255 473255 473255 473255]\n",
      "\n",
      "First few elements of test set:\n",
      "Event IDs: [486708620 486708669 486708930 486708934 486709125]\n",
      "Run IDs: [473255 473255 473255 473255 473255]\n"
     ]
    }
   ],
   "source": [
    "# Sort topo2A training data\n",
    "train_sort_indices = np.lexsort((train_event_id, train_run_id))\n",
    "Topo_2A_train = Topo_2A_train[train_sort_indices]\n",
    "Topo_train_weights = Topo_train_weights[train_sort_indices]\n",
    "train_event_id = train_event_id[train_sort_indices]\n",
    "train_run_id = train_run_id[train_sort_indices]\n",
    "\n",
    "# Sort test data\n",
    "test_sort_indices = np.lexsort((test_event_id, test_run_id))\n",
    "Topo_2A_test = Topo_2A_test[test_sort_indices]\n",
    "Topo_test_weights = Topo_test_weights[test_sort_indices]\n",
    "test_event_id = test_event_id[test_sort_indices]\n",
    "test_run_id = test_run_id[test_sort_indices]\n",
    "\n",
    "print(\"Sorted shapes:\")\n",
    "print(\"Topo_2A_train:\", Topo_2A_train.shape)\n",
    "print(\"Topo_2A_test:\", Topo_2A_test.shape)\n",
    "\n",
    "# Print the first few elements of event and run numbers for train and test sets\n",
    "print(\"\\nFirst few elements of train set:\")\n",
    "print(\"Event IDs:\", train_event_id[:5])\n",
    "print(\"Run IDs:\", train_run_id[:5])\n",
    "\n",
    "print(\"\\nFirst few elements of test set:\")\n",
    "print(\"Event IDs:\", test_event_id[:5])\n",
    "print(\"Run IDs:\", test_run_id[:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "082110a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EB Train set size: 1754041\n",
      "EB Test set size: 511728\n",
      "\n",
      "First few elements of EB train set:\n",
      "Event IDs: [507531069 507544334 507542373 507532437 507539498]\n",
      "Run IDs: [473255 473255 473255 473255 473255]\n",
      "\n",
      "First few elements of EB test set:\n",
      "Event IDs: [448033363 448022581 448026386 448024119 448024409]\n",
      "Run IDs: [473255 473255 473255 473255 473255]\n"
     ]
    }
   ],
   "source": [
    "# Now split my EB data into train and test sets according to the run / event numbers from topo2A\n",
    "\n",
    "# Create sets of (run, event) tuples for train and test data\n",
    "train_set = set(zip(train_run_id, train_event_id))\n",
    "test_set = set(zip(test_run_id, test_event_id))\n",
    "\n",
    "# Initialize dictionaries for EB train and test data\n",
    "EB_train = {key: [] for key in datasets['EB'].keys()}\n",
    "EB_test = {key: [] for key in datasets['EB'].keys()}\n",
    "\n",
    "# Iterate through the EB dataset\n",
    "for i in range(len(datasets['EB']['run_numbers'])):\n",
    "    run = datasets['EB']['run_numbers'][i]\n",
    "    event = datasets['EB']['event_numbers'][i]\n",
    "    \n",
    "    if (run, event) in train_set:\n",
    "        for key in datasets['EB'].keys():\n",
    "            EB_train[key].append(datasets['EB'][key][i])\n",
    "    else:\n",
    "        for key in datasets['EB'].keys():\n",
    "            EB_test[key].append(datasets['EB'][key][i])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "for key in EB_train.keys():\n",
    "    EB_train[key] = np.array(EB_train[key])\n",
    "    EB_test[key] = np.array(EB_test[key])\n",
    "\n",
    "# Print some information about the split\n",
    "print(\"EB Train set size:\", len(EB_train['run_numbers']))\n",
    "print(\"EB Test set size:\", len(EB_test['run_numbers']))\n",
    "\n",
    "# Print the first few elements of event and run numbers for EB train and test sets\n",
    "print(\"\\nFirst few elements of EB train set:\")\n",
    "print(\"Event IDs:\", EB_train['event_numbers'][:5])\n",
    "print(\"Run IDs:\", EB_train['run_numbers'][:5])\n",
    "\n",
    "print(\"\\nFirst few elements of EB test set:\")\n",
    "print(\"Event IDs:\", EB_test['event_numbers'][:5])\n",
    "print(\"Run IDs:\", EB_test['run_numbers'][:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "784ec8f2-9a6c-4d4b-90b8-a0522ce6d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few elements of ordered EB train set:\n",
      "Event IDs: [480857418 480857460 480857610 480857642 480857751]\n",
      "Run IDs: [473255 473255 473255 473255 473255]\n",
      "\n",
      "First few elements of ordered EB test set:\n",
      "Event IDs: [448012770 448012970 448013451 448013665 448014097]\n",
      "Run IDs: [473255 473255 473255 473255 473255]\n"
     ]
    }
   ],
   "source": [
    "# Order EB train data using lexsort\n",
    "train_order = np.lexsort((EB_train['event_numbers'], EB_train['run_numbers']))\n",
    "for key in EB_train.keys():\n",
    "   EB_train[key] = EB_train[key][train_order]\n",
    "\n",
    "# Order EB test data using lexsort\n",
    "test_order = np.lexsort((EB_test['event_numbers'], EB_test['run_numbers']))\n",
    "for key in EB_test.keys():\n",
    "    EB_test[key] = EB_test[key][test_order]\n",
    "\n",
    "# Print the first few elements of ordered event and run numbers for EB train and test sets\n",
    "print(\"\\nFirst few elements of ordered EB train set:\")\n",
    "print(\"Event IDs:\", EB_train['event_numbers'][:5])\n",
    "print(\"Run IDs:\", EB_train['run_numbers'][:5])\n",
    "\n",
    "print(\"\\nFirst few elements of ordered EB test set:\")\n",
    "print(\"Event IDs:\", EB_test['event_numbers'][:5])\n",
    "print(\"Run IDs:\", EB_test['run_numbers'][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75044a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"now remove events from the topo2A data that dont exist in EB\"\"\"\n",
    "\n",
    "# Create sets of (run_number, event_number) pairs for EB and topo2A\n",
    "EB_pairs = set(zip(EB_train['run_numbers'], EB_train['event_numbers']))\n",
    "EB_pairs.update(zip(EB_test['run_numbers'], EB_test['event_numbers']))\n",
    "\n",
    "topo2A_pairs = set(zip(train_run_id, train_event_id))\n",
    "topo2A_pairs.update(zip(test_run_id, test_event_id))\n",
    "\n",
    "# Find pairs that are in topo2A but not in EB, and pairs that are in EB but not in topo2A\n",
    "pairs_to_remove = (topo2A_pairs - EB_pairs) | (EB_pairs - topo2A_pairs)\n",
    "\n",
    "# Function to remove pairs from EB dataset\n",
    "# def remove_pairs_EB(dataset, pairs_to_remove):\n",
    "#     mask = np.ones(len(dataset['run_numbers']), dtype=bool)\n",
    "#     for run, event in pairs_to_remove:\n",
    "#         mask &= ~((dataset['run_numbers'] == run) & (dataset['event_numbers'] == event))\n",
    "#     return {key: dataset[key][mask] for key in dataset.keys()}\n",
    "\n",
    "# # Function to remove pairs from topo2A arrays\n",
    "# def remove_pairs_topo2A(run_number, event_number, data, pairs_to_remove):\n",
    "#     mask = np.ones(len(run_number), dtype=bool)\n",
    "#     for run, event in pairs_to_remove:\n",
    "#         mask &= ~((run_number == run) & (event_number == event))\n",
    "#     return run_number[mask], event_number[mask], data[mask]\n",
    "\n",
    "def remove_pairs_EB(dataset, pairs_to_remove):\n",
    "    # Create a mask that will keep all elements initially\n",
    "    mask = np.ones(len(dataset['run_numbers']), dtype=bool)\n",
    "\n",
    "    # Iterate over the dataset and mark elements to remove based on pairs\n",
    "    for i in range(len(dataset['run_numbers'])):\n",
    "        if (dataset['run_numbers'][i], dataset['event_numbers'][i]) in pairs_to_remove:\n",
    "            mask[i] = False  # Mark to remove\n",
    "\n",
    "    # Apply the mask to all dataset keys\n",
    "    return {key: dataset[key][mask] for key in dataset}\n",
    "\n",
    "def remove_pairs_topo2A(run_number, event_number, data, pairs_to_remove):\n",
    "    # Create a mask that will keep all elements initially\n",
    "    mask = np.ones(len(run_number), dtype=bool)\n",
    "\n",
    "    # Iterate over run_number and event_number and mark elements to remove based on pairs\n",
    "    for i in range(len(run_number)):\n",
    "        if (run_number[i], event_number[i]) in pairs_to_remove:\n",
    "            mask[i] = False  # Mark to remove\n",
    "\n",
    "    # Apply the mask and return the filtered data\n",
    "    return run_number[mask], event_number[mask], data[mask]\n",
    "\n",
    "# Remove pairs from topo2A and EB train and test sets\n",
    "topo2A_train_run_number, topo2A_train_event_number, topo2A_train = remove_pairs_topo2A(train_run_id, train_event_id, Topo_2A_train, pairs_to_remove)\n",
    "topo2A_test_run_number, topo2A_test_event_number, topo2A_test = remove_pairs_topo2A(test_run_id, test_event_id, Topo_2A_test, pairs_to_remove)\n",
    "EB_train = remove_pairs_EB(EB_train, pairs_to_remove)\n",
    "EB_test = remove_pairs_EB(EB_test, pairs_to_remove)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19080fc9-335c-44cd-8167-8ac2faf4df75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480857418 473255\n",
      "486708620 473255\n",
      "480857418 473255\n",
      "486708620 473255\n"
     ]
    }
   ],
   "source": [
    "print(topo2A_train_event_number[0], topo2A_train_run_number[0])\n",
    "print(topo2A_test_event_number[0], topo2A_test_run_number[0])\n",
    "print(EB_train['event_numbers'][0], EB_train['run_numbers'][0])\n",
    "print(EB_test['event_numbers'][0], EB_test['run_numbers'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a224a03-25a6-40e7-b83c-32f5559528f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616549693 473255\n",
      "700376589 473255\n",
      "616549693 473255\n",
      "700376589 473255\n"
     ]
    }
   ],
   "source": [
    "idx = 345254\n",
    "print(topo2A_train_event_number[idx], topo2A_train_run_number[idx])\n",
    "print(topo2A_test_event_number[idx], topo2A_test_run_number[idx])\n",
    "print(EB_train['event_numbers'][idx], EB_train['run_numbers'][idx])\n",
    "print(EB_test['event_numbers'][idx], EB_test['run_numbers'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "515a0372-8264-4278-b934-cd8413a7d8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all events in topo2A_train matched!\n",
      "all events in topo2A_test matched!\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(topo2A_train_event_number)):\n",
    "    assert (topo2A_train_event_number[idx] == EB_train['event_numbers'][idx]), f'train error at index {i}'\n",
    "\n",
    "print(f'all events in topo2A_train matched!')\n",
    "\n",
    "for i in range(len(topo2A_test_event_number)):\n",
    "    assert (topo2A_test_event_number[idx] == EB_test['event_numbers'][idx]), f'test error at index {i}'\n",
    "\n",
    "print(f'all events in topo2A_test matched!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "420579de-6792-4c97-bb58-9617e8f224c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now add the additional run to the EB datasets dict\n",
    "datasets['EB_train'] = EB_train\n",
    "datasets['EB_test'] = EB_test\n",
    "\n",
    "datasets = combine_data(datasets, tags_to_combine=['EB_test', 'EB_482596_0'], new_tag='EB_test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c45fa9c-fa70-481d-bf1f-48921175820f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event index looks good :)\n"
     ]
    }
   ],
   "source": [
    "# Now add the additional run to the topo2A data\n",
    "Topo_2A_new, Topo_2B_new, Topo_3A_new, pass_L1_unprescaled_new, pass_HLT_unprescaled_new, EB_weights_new, event_id_new, run_id_new = load_and_process_normal_data('/eos/home-m/mmcohen/ntuples/EB_h5_10-06-2024/EB_482596_0_10-05-2024.h5')\n",
    "\n",
    "topo2A_test_run_number = np.concatenate((topo2A_test_run_number, run_id_new), axis=0)\n",
    "topo2A_test_event_number = np.concatenate((topo2A_test_event_number, event_id_new), axis=0)\n",
    "topo2A_test = np.concatenate((topo2A_test, Topo_2A_new), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d83c3a55-d71e-4a69-87e3-f8c7feed8675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event index looks good :)\n",
      "event index looks good :)\n"
     ]
    }
   ],
   "source": [
    "# Load the HLTnalg_L1all run\n",
    "Topo_2A_L1all, Topo_2B_L1all, Topo_3A_L1all, pass_L1_unprescaled_L1all, pass_HLT_unprescaled_L1all, EB_weights_L1all, event_id_L1all, run_id_L1all = load_and_process_normal_data('/eos/home-m/mmcohen/ntuples/EB_h5_10-06-2024/EB_475341_0_10-05-2024.h5')\n",
    "Topo_2A_L1all2, Topo_2B_L1all2, Topo_3A_L1all2, pass_L1_unprescaled_L1all2, pass_HLT_unprescaled_L1all2, EB_weights_L1all2, event_id_L1all2, run_id_L1all2 = load_and_process_normal_data('/eos/home-m/mmcohen/ntuples/EB_h5_10-06-2024/EB_475341_1_10-05-2024.h5')\n",
    "\n",
    "Topo_2A_L1all = np.concatenate((Topo_2A_L1all, Topo_2A_L1all2), axis=0)\n",
    "event_id_L1all = np.concatenate((event_id_L1all, event_id_L1all2), axis=0)\n",
    "run_id_L1all = np.concatenate((run_id_L1all, run_id_L1all2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "472a0666-4a78-4cbb-a22e-e93252e1c8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all events in topo2A_train matched!\n",
      "all events in topo2A_test matched!\n",
      "all events in HLT_noalg_eb_L1All matched!\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(topo2A_train_event_number)):\n",
    "    assert (topo2A_train_event_number[idx] == EB_train['event_numbers'][idx]), f'train error at index {i}'\n",
    "\n",
    "print(f'all events in topo2A_train matched!')\n",
    "\n",
    "for i in range(len(topo2A_test_event_number)):\n",
    "    assert (topo2A_test_event_number[idx] == EB_test['event_numbers'][idx]), f'test error at index {i}'\n",
    "\n",
    "print(f'all events in topo2A_test matched!')\n",
    "\n",
    "for i in range(len(event_id_L1all)):\n",
    "    assert (event_id_L1all[idx] == datasets['HLT_noalg_eb_L1All']['event_numbers'][idx]), f'test error at index {i}'\n",
    "\n",
    "print(f'all events in HLT_noalg_eb_L1All matched!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5021899f-0539-4b24-aff1-5e4f516eed97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42fb9b29-6742-446c-a390-d35b74fc9411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['HLT_MET', 'HLT_electrons', 'HLT_jets', 'HLT_muons', 'HLT_photons', 'L1_MET', 'L1_eFex_taus', 'L1_egammas', 'L1_jFexLR_jets', 'L1_jFexSR_jets', 'L1_jFex_taus', 'L1_muons', 'LRT_electrons', 'LRT_muons', 'pass_HLT_unprescaled', 'pass_L1_unprescaled']>\n",
      "<KeysViewHDF5 ['HLT_MET', 'HLT_electrons', 'HLT_jets', 'HLT_muons', 'HLT_photons', 'L1_MET', 'L1_eFex_taus', 'L1_egammas', 'L1_jFexLR_jets', 'L1_jFexSR_jets', 'L1_jFex_taus', 'L1_muons', 'LRT_electrons', 'LRT_muons', 'pass_HLT_unprescaled', 'pass_L1_unprescaled']>\n",
      "<KeysViewHDF5 ['HLT_MET', 'HLT_electrons', 'HLT_jets', 'HLT_muons', 'HLT_photons', 'L1_MET', 'L1_eFex_taus', 'L1_egammas', 'L1_jFexLR_jets', 'L1_jFexSR_jets', 'L1_jFex_taus', 'L1_muons', 'LRT_electrons', 'LRT_muons', 'pass_HLT_unprescaled', 'pass_L1_unprescaled']>\n",
      "<KeysViewHDF5 ['HLT_MET', 'HLT_electrons', 'HLT_jets', 'HLT_muons', 'HLT_photons', 'L1_MET', 'L1_eFex_taus', 'L1_egammas', 'L1_jFexLR_jets', 'L1_jFexSR_jets', 'L1_jFex_taus', 'L1_muons', 'LRT_electrons', 'LRT_muons', 'pass_HLT_unprescaled', 'pass_L1_unprescaled']>\n",
      "<KeysViewHDF5 ['HLT_MET', 'HLT_electrons', 'HLT_jets', 'HLT_muons', 'HLT_photons', 'L1_MET', 'L1_eFex_taus', 'L1_egammas', 'L1_jFexLR_jets', 'L1_jFexSR_jets', 'L1_jFex_taus', 'L1_muons', 'LRT_electrons', 'LRT_muons', 'pass_HLT_unprescaled', 'pass_L1_unprescaled']>\n",
      "<KeysViewHDF5 ['HLT_MET', 'HLT_electrons', 'HLT_jets', 'HLT_muons', 'HLT_photons', 'L1_MET', 'L1_eFex_taus', 'L1_egammas', 'L1_jFexLR_jets', 'L1_jFexSR_jets', 'L1_jFex_taus', 'L1_muons', 'LRT_electrons', 'LRT_muons', 'pass_HLT_unprescaled', 'pass_L1_unprescaled']>\n",
      "<KeysViewHDF5 ['HLT_MET', 'HLT_electrons', 'HLT_jets', 'HLT_muons', 'HLT_photons', 'L1_MET', 'L1_eFex_taus', 'L1_egammas', 'L1_jFexLR_jets', 'L1_jFexSR_jets', 'L1_jFex_taus', 'L1_muons', 'LRT_electrons', 'LRT_muons', 'pass_HLT_unprescaled', 'pass_L1_unprescaled']>\n",
      "<KeysViewHDF5 ['HLT_MET', 'HLT_electrons', 'HLT_jets', 'HLT_muons', 'HLT_photons', 'L1_MET', 'L1_eFex_taus', 'L1_egammas', 'L1_jFexLR_jets', 'L1_jFexSR_jets', 'L1_jFex_taus', 'L1_muons', 'LRT_electrons', 'LRT_muons', 'pass_HLT_unprescaled', 'pass_L1_unprescaled']>\n",
      "<KeysViewHDF5 ['HLT_MET', 'HLT_electrons', 'HLT_jets', 'HLT_muons', 'HLT_photons', 'L1_MET', 'L1_eFex_taus', 'L1_egammas', 'L1_jFexLR_jets', 'L1_jFexSR_jets', 'L1_jFex_taus', 'L1_muons', 'LRT_electrons', 'LRT_muons', 'pass_HLT_unprescaled', 'pass_L1_unprescaled']>\n"
     ]
    }
   ],
   "source": [
    "# Load anomalous data\n",
    "Topo_2A_HHbbtt, _, _, pass_L1_HHbbtt = load_and_process_anomalous_data('/eos/home-m/mmcohen/ntuples/MC_07-17-2024/HHbbttHadHad_07-10-2024.h5')\n",
    "Topo_2A_A14, _, _, pass_L1_A14 = load_and_process_anomalous_data('/eos/home-m/mmcohen/ntuples/MC_07-17-2024/A14N23LO_07-17-2024.h5')\n",
    "Topo_2A_HAHMggf, _, _, pass_L1_HAHMggf = load_and_process_anomalous_data('/eos/home-m/mmcohen/ntuples/MC_07-17-2024/HAHMggfZdZd2l2nu_07-17-2024.h5')\n",
    "Topo_2A_qqa, _, _, pass_L1_qqa = load_and_process_anomalous_data('/eos/home-m/mmcohen/ntuples/MC_07-17-2024/qqa_07-17-2024.h5')\n",
    "Topo_2A_Zprime, _, _, pass_L1_Zprime = load_and_process_anomalous_data('/eos/home-m/mmcohen/ntuples/MC_07-17-2024/Zprime2EJs_07-17-2024.h5')\n",
    "Topo_2A_ZZ4lep, _, _, pass_L1_ZZ4lep = load_and_process_anomalous_data('/eos/home-m/mmcohen/ntuples/MC_07-17-2024/ZZ4lep_07-17-2024.h5')\n",
    "Topo_2A_jz1, _, _, pass_L1_jz1 = load_and_process_anomalous_data('/eos/home-m/mmcohen/ntuples/MC_07-17-2024/jjJZ1_07-17-2024.h5')\n",
    "Topo_2A_jz2, _, _, pass_L1_jz2 = load_and_process_anomalous_data('/eos/home-m/mmcohen/ntuples/MC_07-17-2024/jjJZ2_07-17-2024.h5')\n",
    "Topo_2A_jz4, _, _, pass_L1_jz4 = load_and_process_anomalous_data('/eos/home-m/mmcohen/ntuples/MC_07-17-2024/jjJZ4_07-17-2024.h5')\n",
    "\n",
    "# Topo_2A_HLT_passed = Topo_2A_test_signal[HLT_pass_test==1]\n",
    "# Topo_2A_L1_passed = Topo_2A_test_signal[L1_pass_test==1]\n",
    "# Topo_2A_just_L1_passed = Topo_2A_L1_passed[HLT_pass_test[L1_pass_test==1]==0]\n",
    "\n",
    "# Topo_2A_HHbbtt_pure = Topo_2A_HHbbtt[pass_L1_HHbbtt==0]\n",
    "# Topo_2A_A14_pure = Topo_2A_A14[pass_L1_A14==0]\n",
    "# Topo_2A_HAHMggf_pure = Topo_2A_HAHMggf[pass_L1_HAHMggf==0]\n",
    "# Topo_2A_qqa_pure = Topo_2A_qqa[pass_L1_qqa==0]\n",
    "# Topo_2A_Zprime_pure = Topo_2A_Zprime[pass_L1_Zprime==0]\n",
    "# Topo_2A_ZZ4lep_pure = Topo_2A_ZZ4lep[pass_L1_ZZ4lep==0]\n",
    "# Topo_2A_jz1_pure = Topo_2A_jz1[pass_L1_jz1==0]\n",
    "# Topo_2A_jz2_pure = Topo_2A_jz2[pass_L1_jz2==0]\n",
    "# Topo_2A_jz4_pure = Topo_2A_jz4[pass_L1_jz4==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0116f58e-ae21-4e0a-b06e-b1b29fbb7a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "targz_path = './trained_models/software_model_BESTOFLONGRUN.tar.gz'  # Replace with the actual path to your .tar.gz file\n",
    "model_name = '2A_AE_model_V9_BESTOFLONGRUN'\n",
    "model = load_model_from_targz(targz_path, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa0bd298-4926-4891-a151-7040616f765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define signals and signal names\n",
    "signals = [\n",
    "    topo2A_test,\n",
    "    #Topo_2A_HLT_passed,\n",
    "    #Topo_2A_L1_passed,\n",
    "    Topo_2A_HHbbtt[0:100000],\n",
    "    Topo_2A_jz1[0:100000],\n",
    "    Topo_2A_jz2[0:100000],\n",
    "    Topo_2A_jz4[0:100000],\n",
    "    Topo_2A_A14[0:100000],\n",
    "    Topo_2A_HAHMggf[0:100000],\n",
    "    Topo_2A_qqa[0:100000],\n",
    "    Topo_2A_Zprime[0:100000],\n",
    "    Topo_2A_ZZ4lep[0:100000],\n",
    "    topo2A_train,\n",
    "    Topo_2A_L1all\n",
    "]\n",
    "\n",
    "# signal_names = [\n",
    "#     \"Topo_2A_test_signal\",\n",
    "#     \"Topo_2A_HLT_passed\",\n",
    "#     \"Topo_2A_L1_passed\",\n",
    "#     \"Topo_2A_HHbbtt\",\n",
    "#     \"Topo_2A_jz1\",\n",
    "#     \"Topo_2A_jz2\",\n",
    "#     \"Topo_2A_jz4\",\n",
    "#     \"Topo_2A_A14\",\n",
    "#     \"Topo_2A_HAHMggf\",\n",
    "#     \"Topo_2A_qqa\",\n",
    "#     \"Topo_2A_Zprime\",\n",
    "#     \"Topo_2A_ZZ4lep\"\n",
    "# ]\n",
    "\n",
    "signal_names = [\n",
    "    \"EB_test2\",\n",
    "    \"HHbbttHadHad\",\n",
    "    \"jjJZ1\",\n",
    "    \"jjJZ2\",\n",
    "    \"jjJZ4\",\n",
    "    \"A14N23LO\",\n",
    "    \"HAHMggfZdZd2l2nu\",\n",
    "    \"qqa\",\n",
    "    \"Zprime2EJs\",\n",
    "    \"ZZ4lep\",\n",
    "    \"EB_train\",\n",
    "    \"HLT_noalg_eb_L1All\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c44d732-1c88-40b9-8f9a-c837144a9ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43698/43698 [==============================] - 137s 3ms/step\n",
      "3125/3125 [==============================] - 10s 3ms/step\n",
      "3125/3125 [==============================] - 10s 3ms/step\n",
      "3125/3125 [==============================] - 10s 3ms/step\n",
      "3125/3125 [==============================] - 10s 3ms/step\n",
      "313/313 [==============================] - 2s 3ms/step\n",
      "2188/2188 [==============================] - 7s 3ms/step\n",
      "1563/1563 [==============================] - 5s 3ms/step\n",
      "3125/3125 [==============================] - 10s 3ms/step\n",
      "3125/3125 [==============================] - 10s 3ms/step\n",
      "54814/54814 [==============================] - 185s 3ms/step\n",
      "34604/34604 [==============================] - 117s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and save triggers\n",
    "for signal, signal_name in zip(signals, signal_names):\n",
    "    predictions = model.predict(signal)\n",
    "    AD_score = np.mean(np.square(predictions), axis=1)\n",
    "    datasets[signal_name]['topo2A_AD_scores'] = AD_score\n",
    "    # threshold = 0.0535068511962890625\n",
    "    # above_threshold = AD_score > threshold\n",
    "    # trigger = above_threshold.astype(int)\n",
    "    \n",
    "    # # Save the trigger for each signal\n",
    "    # np.save(f'{signal_name}_trigger.npy', trigger)\n",
    "\n",
    "#print(\"Predictions and triggers have been saved for all signals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "47ca50d0-6b6d-43db-9bce-a1ed5ef8b5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A14N23LO:\n",
      "HLT_data: (10000, 16, 3)\n",
      "L1_data: (10000, 16, 3)\n",
      "passL1: (10000,)\n",
      "passHLT: (10000,)\n",
      "weights: (10000,)\n",
      "topo2A_AD_scores: (10000,)\n",
      "\n",
      "HAHMggfZdZd2l2nu:\n",
      "HLT_data: (70000, 16, 3)\n",
      "L1_data: (70000, 16, 3)\n",
      "passL1: (70000,)\n",
      "passHLT: (70000,)\n",
      "weights: (70000,)\n",
      "topo2A_AD_scores: (70000,)\n",
      "\n",
      "HHbbttHadHad:\n",
      "HLT_data: (100000, 16, 3)\n",
      "L1_data: (100000, 16, 3)\n",
      "passL1: (100000,)\n",
      "passHLT: (100000,)\n",
      "weights: (100000,)\n",
      "topo2A_AD_scores: (100000,)\n",
      "\n",
      "ZZ4lep:\n",
      "HLT_data: (100000, 16, 3)\n",
      "L1_data: (100000, 16, 3)\n",
      "passL1: (100000,)\n",
      "passHLT: (100000,)\n",
      "weights: (100000,)\n",
      "topo2A_AD_scores: (100000,)\n",
      "\n",
      "Zprime2EJs:\n",
      "HLT_data: (100000, 16, 3)\n",
      "L1_data: (100000, 16, 3)\n",
      "passL1: (100000,)\n",
      "passHLT: (100000,)\n",
      "weights: (100000,)\n",
      "topo2A_AD_scores: (100000,)\n",
      "\n",
      "jjJZ1:\n",
      "HLT_data: (100000, 16, 3)\n",
      "L1_data: (100000, 16, 3)\n",
      "passL1: (100000,)\n",
      "passHLT: (100000,)\n",
      "weights: (100000,)\n",
      "topo2A_AD_scores: (100000,)\n",
      "\n",
      "jjJZ2:\n",
      "HLT_data: (100000, 16, 3)\n",
      "L1_data: (100000, 16, 3)\n",
      "passL1: (100000,)\n",
      "passHLT: (100000,)\n",
      "weights: (100000,)\n",
      "topo2A_AD_scores: (100000,)\n",
      "\n",
      "jjJZ4:\n",
      "HLT_data: (100000, 16, 3)\n",
      "L1_data: (100000, 16, 3)\n",
      "passL1: (100000,)\n",
      "passHLT: (100000,)\n",
      "weights: (100000,)\n",
      "topo2A_AD_scores: (100000,)\n",
      "\n",
      "qqa:\n",
      "HLT_data: (50000, 16, 3)\n",
      "L1_data: (50000, 16, 3)\n",
      "passL1: (50000,)\n",
      "passHLT: (50000,)\n",
      "weights: (50000,)\n",
      "topo2A_AD_scores: (50000,)\n",
      "\n",
      "HLT_noalg_eb_L1All:\n",
      "HLT_data: (1107321, 16, 3)\n",
      "L1_data: (1107321, 16, 3)\n",
      "passL1: (1107321,)\n",
      "passHLT: (1107321,)\n",
      "weights: (1107321,)\n",
      "event_numbers: (1107321,)\n",
      "run_numbers: (1107321,)\n",
      "pileups: (1107321,)\n",
      "topo2A_AD_scores: (1107321,)\n",
      "\n",
      "EB:\n",
      "HLT_data: (2265769, 16, 3)\n",
      "L1_data: (2265769, 16, 3)\n",
      "passL1: (2265769,)\n",
      "passHLT: (2265769,)\n",
      "weights: (2265769,)\n",
      "event_numbers: (2265769,)\n",
      "run_numbers: (2265769,)\n",
      "pileups: (2265769,)\n",
      "\n",
      "EB_train:\n",
      "HLT_data: (1754041, 16, 3)\n",
      "L1_data: (1754041, 16, 3)\n",
      "passL1: (1754041,)\n",
      "passHLT: (1754041,)\n",
      "weights: (1754041,)\n",
      "event_numbers: (1754041,)\n",
      "run_numbers: (1754041,)\n",
      "pileups: (1754041,)\n",
      "topo2A_AD_scores: (1754041,)\n",
      "\n",
      "EB_test2:\n",
      "HLT_data: (1398336, 16, 3)\n",
      "L1_data: (1398336, 16, 3)\n",
      "passL1: (1398336,)\n",
      "passHLT: (1398336,)\n",
      "weights: (1398336,)\n",
      "event_numbers: (1398336,)\n",
      "run_numbers: (1398336,)\n",
      "pileups: (1398336,)\n",
      "topo2A_AD_scores: (1398336,)\n"
     ]
    }
   ],
   "source": [
    "# Display the structure of updated datasets\n",
    "for tag, data in datasets.items():\n",
    "    print(f'\\n{tag}:')\n",
    "    for key, value in data.items():\n",
    "        print(f'{key}: {value.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6cd15be5-1e36-44ac-ba08-d5bcb232895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_subdicts_to_h5(main_dict, save_dir):\n",
    "    \"\"\"\n",
    "    Saves each sub-dictionary of NumPy arrays in the main_dict to separate HDF5 files.\n",
    "    \n",
    "    Args:\n",
    "        main_dict (dict): A dictionary of dictionaries where the innermost values are NumPy arrays.\n",
    "        save_dir (str): The directory where the HDF5 files will be saved.\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    for sub_dict_name, sub_dict in main_dict.items():\n",
    "        file_path = os.path.join(save_dir, f\"{sub_dict_name}.h5\")\n",
    "        with h5py.File(file_path, 'w') as f:\n",
    "            for key, arr in sub_dict.items():\n",
    "                f.create_dataset(key, data=arr)\n",
    "        print(f\"Saved {sub_dict_name} to {file_path}\")\n",
    "\n",
    "\n",
    "def load_subdicts_from_h5(save_dir):\n",
    "    \"\"\"\n",
    "    Loads sub-dictionaries of NumPy arrays from HDF5 files in a directory and reconstructs the original structure.\n",
    "    \n",
    "    Args:\n",
    "        save_dir (str): The directory where the HDF5 files are stored.\n",
    "    \n",
    "    Returns:\n",
    "        main_dict (dict): A dictionary of dictionaries where the innermost values are NumPy arrays.\n",
    "    \"\"\"\n",
    "    main_dict = {}\n",
    "    \n",
    "    for filename in os.listdir(save_dir):\n",
    "        if filename.endswith(\".h5\"):\n",
    "            sub_dict_name = os.path.splitext(filename)[0]\n",
    "            file_path = os.path.join(save_dir, filename)\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                sub_dict = {key: np.array(f[key]) for key in f}\n",
    "            main_dict[sub_dict_name] = sub_dict\n",
    "            print(f\"Loaded {sub_dict_name} from {file_path}\")\n",
    "    \n",
    "    return main_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c9e3fcd-aad4-4cbf-9e5f-591d7d03a828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved A14N23LO to ./h5_ntuples/A14N23LO.h5\n",
      "Saved HAHMggfZdZd2l2nu to ./h5_ntuples/HAHMggfZdZd2l2nu.h5\n",
      "Saved HHbbttHadHad to ./h5_ntuples/HHbbttHadHad.h5\n",
      "Saved ZZ4lep to ./h5_ntuples/ZZ4lep.h5\n",
      "Saved Zprime2EJs to ./h5_ntuples/Zprime2EJs.h5\n",
      "Saved jjJZ1 to ./h5_ntuples/jjJZ1.h5\n",
      "Saved jjJZ2 to ./h5_ntuples/jjJZ2.h5\n",
      "Saved jjJZ4 to ./h5_ntuples/jjJZ4.h5\n",
      "Saved qqa to ./h5_ntuples/qqa.h5\n",
      "Saved HLT_noalg_eb_L1All to ./h5_ntuples/HLT_noalg_eb_L1All.h5\n",
      "Saved EB to ./h5_ntuples/EB.h5\n",
      "Saved EB_train to ./h5_ntuples/EB_train.h5\n",
      "Saved EB_test2 to ./h5_ntuples/EB_test2.h5\n"
     ]
    }
   ],
   "source": [
    "save_subdicts_to_h5(datasets, save_dir='./h5_ntuples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cc6d7117-0277-4285-91e8-23da7c234b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded A14N23LO from ./h5_ntuples/A14N23LO.h5\n",
      "Loaded EB from ./h5_ntuples/EB.h5\n",
      "Loaded EB_test2 from ./h5_ntuples/EB_test2.h5\n",
      "Loaded EB_train from ./h5_ntuples/EB_train.h5\n",
      "Loaded HAHMggfZdZd2l2nu from ./h5_ntuples/HAHMggfZdZd2l2nu.h5\n",
      "Loaded HHbbttHadHad from ./h5_ntuples/HHbbttHadHad.h5\n",
      "Loaded HLT_noalg_eb_L1All from ./h5_ntuples/HLT_noalg_eb_L1All.h5\n",
      "Loaded ZZ4lep from ./h5_ntuples/ZZ4lep.h5\n",
      "Loaded Zprime2EJs from ./h5_ntuples/Zprime2EJs.h5\n",
      "Loaded jjJZ1 from ./h5_ntuples/jjJZ1.h5\n",
      "Loaded jjJZ2 from ./h5_ntuples/jjJZ2.h5\n",
      "Loaded jjJZ4 from ./h5_ntuples/jjJZ4.h5\n",
      "Loaded qqa from ./h5_ntuples/qqa.h5\n"
     ]
    }
   ],
   "source": [
    "datasets = load_subdicts_from_h5('./h5_ntuples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77711b05-79ea-4da1-9268-fb54943fdc17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00fc8e1-a568-44c1-803f-50f5f5610368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do a few checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "10b4cf3a-73ba-4816-831d-ff8ac22398d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting checks of test data!\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "\n",
      "---\n",
      "Starting check for index 0\n",
      "topo 2A event:\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.09667968 -1.34375006 -3.37475777  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01215075  0.48962492]\n",
      "\n",
      "EB_test event:\n",
      "[[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [10.5        -2.16250014 -2.69980621]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 9.89999962 -2.1500001  -2.69980621]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 9.95389366  0.          0.39169994]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.0029272090177983046\n",
      "AD score calculate by running the event through the model:\n",
      "0.0029272090177983046\n",
      "\n",
      "---\n",
      "Starting check for index 2314\n",
      "topo 2A event:\n",
      "[ 0.29296875 -0.21875     0.30679617  0.20703126 -1.53125003 -3.86563152\n",
      "  0.18750001 -1.40625     2.02485457  0.18750001 -1.34375006  1.77941769\n",
      "  0.17773438 -0.15625    -1.53398082  0.15820313  0.96874997  2.2702916\n",
      "  0.12011719 -0.15625     0.30679615  0.09179687 -1.28125012  1.65669918\n",
      "  0.0625     -0.09375    -1.65669903  0.05078125 -0.34375001 -1.77941784\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01277488 -2.94524312]\n",
      "\n",
      "EB_test event:\n",
      "[[30.         -0.34999999  0.24543694]\n",
      " [21.20000076 -2.45000005 -3.09250522]\n",
      " [19.20000076 -2.25        1.61988366]\n",
      " [19.20000076 -2.1500001   1.42353415]\n",
      " [18.20000076 -0.25       -1.22718465]\n",
      " [16.20000076  1.54999995  1.81623328]\n",
      " [10.         -2.06250024  1.32535934]\n",
      " [ 9.39999962 -0.28750002  0.24543692]\n",
      " [ 4.69999981 -0.1875     -1.32535923]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [12.30000019 -0.25        0.24543692]\n",
      " [ 9.39999962 -2.05000019  1.32535934]\n",
      " [ 6.4000001  -0.15000001 -1.32535923]\n",
      " [10.4651804   0.         -2.3561945 ]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.005060174502432346\n",
      "AD score calculate by running the event through the model:\n",
      "0.005060174502432346\n",
      "\n",
      "---\n",
      "Starting check for index 132445\n",
      "topo 2A event:\n",
      "[ 0.38476564 -0.96874997 -1.04310691  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.20605469 -1.03125006 -1.1658255   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01613731  3.54274392]\n",
      "\n",
      "EB_test event:\n",
      "[[39.40000153 -1.54999995 -0.83448553]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [26.5        -1.63750005 -0.9326604 ]\n",
      " [ 3.0999999   0.36250001 -1.52170885]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [21.10000038 -1.6500001  -0.9326604 ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [13.21968269  0.          2.83419514]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.003926201723515987\n",
      "AD score calculate by running the event through the model:\n",
      "0.003926201723515987\n",
      "\n",
      "---\n",
      "Starting check for index -1\n",
      "topo 2A event:\n",
      "[ 3.90820324  0.71874999 -1.04310691  2.4589844   0.96874997  1.53398082\n",
      "  1.50390625  0.28124999 -3.74291301  1.37500003 -0.71874999  3.00660223\n",
      "  0.81054688 -0.09375     0.06135924  0.296875   -0.96874997  2.51572847\n",
      "  1.88281253  0.71874999 -1.04310669  1.31249994  0.96874997  1.53398082\n",
      "  0.78613281  0.21875001 -3.74291301  0.52539062 -0.71875006  3.00660223\n",
      "  1.25000006  0.25100388 -3.715958    0.62500003 -1.44493505 -2.25798145\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.09111516 -3.82640213]\n",
      "\n",
      "EB_test event:\n",
      "[[ 4.00200012e+02  1.14999998e+00 -8.34485531e-01]\n",
      " [ 2.51800003e+02  1.54999995e+00  1.22718465e+00]\n",
      " [ 1.54000000e+02  4.49999988e-01 -2.99433041e+00]\n",
      " [ 1.40800003e+02 -1.14999998e+00  2.40528178e+00]\n",
      " [ 8.30000000e+01 -1.50000006e-01  4.90873903e-02]\n",
      " [ 3.03999996e+01 -1.54999995e+00  2.01258278e+00]\n",
      " [ 1.60399994e+02  1.13750005e+00 -8.34485352e-01]\n",
      " [ 9.53000031e+01  1.56250000e+00  1.22718465e+00]\n",
      " [ 4.50000000e+01 -1.11250007e+00  2.40528178e+00]\n",
      " [ 8.00000038e-03  4.01606202e-01 -2.97276640e+00]\n",
      " [ 4.00000019e-03 -2.31189609e+00 -1.80638516e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.92800003e+02  1.14999998e+00 -8.34485352e-01]\n",
      " [ 1.34399994e+02  1.54999995e+00  1.22718465e+00]\n",
      " [ 8.05000000e+01  3.50000024e-01 -2.99433041e+00]\n",
      " [ 7.46415405e+01  0.00000000e+00 -3.06112170e+00]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.028722019866108894\n",
      "AD score calculate by running the event through the model:\n",
      "0.028722019866108894\n",
      "\n",
      "\n",
      "Starting checks of test data!\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "\n",
      "---\n",
      "Starting check for index 0\n",
      "topo 2A event:\n",
      "[ 0.18164063  1.97541714 -2.6874277   0.15234375 -0.78125    -1.77941769\n",
      "  0.1484375   1.40625    -0.1840777   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.08789062 -0.78125007 -1.77941784  0.05371094  1.09375     2.51572847\n",
      "  0.04882812 -1.34375006 -1.65669903  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01529342  1.59917444]\n",
      "\n",
      "EB_test event:\n",
      "[[18.60000038  3.16066742 -2.14994216]\n",
      " [15.60000038 -1.25       -1.42353415]\n",
      " [15.19999981  2.25       -0.14726216]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [10.60000038 -1.26250005 -1.42353427]\n",
      " [ 6.5999999   1.78750002  2.01258278]\n",
      " [ 3.5999999  -1.6875      1.71805847]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 9.         -1.25000012 -1.42353427]\n",
      " [ 5.5         1.75        2.01258278]\n",
      " [ 5.         -2.1500001  -1.32535923]\n",
      " [12.528368    0.          1.27933955]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.003316106041893363\n",
      "AD score calculate by running the event through the model:\n",
      "0.003316106041893363\n",
      "\n",
      "---\n",
      "Starting check for index 2314\n",
      "topo 2A event:\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.00775891 -3.64938855]\n",
      "\n",
      "EB_test event:\n",
      "[[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 4.         -1.1875      3.09250522]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 6.35609961  0.         -2.91951084]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.0013206707080826163\n",
      "AD score calculate by running the event through the model:\n",
      "0.0013206707080826163\n",
      "\n",
      "---\n",
      "Starting check for index 132445\n",
      "topo 2A event:\n",
      "[ 0.28710937  2.31137946  0.74095279  0.21679688 -0.90625003 -3.37475777\n",
      "  0.16796876 -1.74999997  0.6135923   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.10644531 -0.90625003 -3.25203925  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.46875    -0.81042126  3.91380519  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01987161 -3.80393326]\n",
      "\n",
      "EB_test event:\n",
      "[[ 2.93999996e+01  3.69820714e+00  5.92762232e-01]\n",
      " [ 2.22000008e+01 -1.45000005e+00 -2.69980621e+00]\n",
      " [ 1.72000008e+01 -2.79999995e+00  4.90873843e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.62999992e+01 -1.41250002e+00 -2.60163140e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.00000003e-03 -1.29667401e+00  3.13104415e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.08999996e+01 -1.45000005e+00 -2.60163140e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.62788200e+01  0.00000000e+00 -3.04314661e+00]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.005964773241430521\n",
      "AD score calculate by running the event through the model:\n",
      "0.005964773241430521\n",
      "\n",
      "---\n",
      "Starting check for index -1\n",
      "topo 2A event:\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00503309 2.46164843]\n",
      "\n",
      "EB_test event:\n",
      "[[0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [4.12310553 0.         1.96931875]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.0015390323242172599\n",
      "AD score calculate by running the event through the model:\n",
      "0.0015390323242172599\n",
      "\n",
      "\n",
      "Starting checks of HLT_noalg_L1_all data!\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "\n",
      "---\n",
      "Starting check for index 0\n",
      "topo 2A event:\n",
      "[ 0.96874997  0.03125    -2.63844699  0.51562499 -0.28124999  1.53398082\n",
      "  0.2734375   0.84375001  0.55223309  0.25195312 -1.15625001 -1.04310691\n",
      "  0.20507812  0.34375001  2.14757308  0.16406249  0.53125001  2.51572847\n",
      "  0.45605469 -0.03125    -2.63844699  0.14550781 -0.34375001  1.4112623\n",
      "  0.12988281 -0.21875     1.65669918  0.10742188  0.03125     1.65669918\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01916309  0.74383706]\n",
      "\n",
      "EB_test event:\n",
      "[[ 9.91999969e+01  5.00000007e-02 -2.11075759e+00]\n",
      " [ 5.27999992e+01 -4.49999988e-01  1.22718465e+00]\n",
      " [ 2.80000000e+01  1.35000002e+00  4.41786468e-01]\n",
      " [ 2.57999992e+01 -1.85000002e+00 -8.34485531e-01]\n",
      " [ 2.10000000e+01  5.50000012e-01  1.71805847e+00]\n",
      " [ 1.67999992e+01  8.50000024e-01  2.01258278e+00]\n",
      " [ 2.88999996e+01 -3.75000015e-02 -2.11075759e+00]\n",
      " [ 1.91000004e+01  3.75000015e-02 -2.11075759e+00]\n",
      " [ 1.25000000e+01  1.31250012e+00  4.41786468e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 4.67000008e+01 -5.00000007e-02 -2.11075759e+00]\n",
      " [ 1.48999996e+01 -5.50000012e-01  1.12900984e+00]\n",
      " [ 1.33000002e+01 -3.49999994e-01  1.32535934e+00]\n",
      " [ 1.56984072e+01  0.00000000e+00  5.95069647e-01]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.03648671135306358\n",
      "AD score calculate by running the event through the model:\n",
      "0.03648671135306358\n",
      "\n",
      "---\n",
      "Starting check for index 2314\n",
      "topo 2A event:\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  1.09375003 -0.77802531 -3.91376734  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.0240463  -0.90092525]\n",
      "\n",
      "EB_test event:\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 5.00000000e+00 -1.91250002e+00  2.40528178e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 7.00000022e-03 -1.24484050e+00 -3.13101387e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.96987305e+01  0.00000000e+00 -7.20740199e-01]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.0013949056155979633\n",
      "AD score calculate by running the event through the model:\n",
      "0.0013949056155979633\n",
      "\n",
      "---\n",
      "Starting check for index 132445\n",
      "topo 2A event:\n",
      "[ 1.16210938 -1.28124997  1.65669918  0.87890625  0.53125001 -2.2702916\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.67578122 -1.28125012  1.65669918  0.44921875  0.46875    -2.27029189\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.02125286 -1.92040905]\n",
      "\n",
      "EB_test event:\n",
      "[[119.          -2.04999995   1.32535934]\n",
      " [ 90.           0.85000002  -1.81623328]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [ 61.          -2.06250024   1.32535934]\n",
      " [ 48.40000153   0.78749996  -1.81623352]\n",
      " [  6.5999999   -1.98749995   1.42353415]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [ 69.19999695  -2.05000019   1.32535934]\n",
      " [ 46.           0.75        -1.81623352]\n",
      " [  0.           0.           0.        ]\n",
      " [ 17.41034126   0.          -1.53632724]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.007631303276866674\n",
      "AD score calculate by running the event through the model:\n",
      "0.007631303276866674\n",
      "\n",
      "---\n",
      "Starting check for index -1\n",
      "topo 2A event:\n",
      "[ 2.17578128 -1.74999997  3.31339836  0.71484372 -0.15625     1.28854379\n",
      "  0.6660156   1.03124999  0.30679617  0.64453125 -1.34375006 -1.4112623\n",
      "  0.37890624 -0.65624997 -1.77941769  0.28710937 -0.34375001 -2.88388401\n",
      "  0.37304688  1.03124999  0.1840777   0.33007812 -0.15625     1.28854379\n",
      "  0.21484375 -1.34375006 -1.4112626   0.15917968 -0.34375001 -2.7611652\n",
      "  1.09375003 -0.77233903 -2.11102292  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.09023993 -1.26883104]\n",
      "\n",
      "EB_test event:\n",
      "[[ 2.22800003e+02 -2.79999995e+00  2.65071869e+00]\n",
      " [ 7.31999969e+01 -2.50000000e-01  1.03083503e+00]\n",
      " [ 6.81999969e+01  1.64999998e+00  2.45436937e-01]\n",
      " [ 6.60000000e+01 -2.15000010e+00 -1.12900984e+00]\n",
      " [ 3.87999992e+01 -1.04999995e+00 -1.42353415e+00]\n",
      " [ 2.93999996e+01 -5.50000012e-01 -2.30710721e+00]\n",
      " [ 2.40000000e+01 -2.87500024e-01  1.03083503e+00]\n",
      " [ 2.05000000e+01  1.61250007e+00  1.47262156e-01]\n",
      " [ 1.82999992e+01 -1.16250002e+00 -1.32535923e+00]\n",
      " [ 7.00000022e-03 -1.23574245e+00 -1.68881834e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.82000008e+01  1.64999998e+00  1.47262156e-01]\n",
      " [ 3.37999992e+01 -2.50000000e-01  1.03083503e+00]\n",
      " [ 2.20000000e+01 -2.15000010e+00 -1.12901008e+00]\n",
      " [ 7.39245529e+01  0.00000000e+00 -1.01506484e+00]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.010364829562604427\n",
      "AD score calculate by running the event through the model:\n",
      "0.010364829562604427\n"
     ]
    }
   ],
   "source": [
    "idxs = [0, 2314, 132445, -1]\n",
    "\n",
    "\n",
    "# Test data ------------------\n",
    "print(f'\\n\\nStarting checks of test data!\\n\\n')\n",
    "\n",
    "# run these events through the model\n",
    "predictions = model.predict(topo2A_test[idxs])\n",
    "_AD_scores = np.mean(np.square(predictions), axis=1)\n",
    "\n",
    "for i, idx in enumerate(idxs):\n",
    "    print(f'\\n---\\nStarting check for index {idx}')\n",
    "    print(f'topo 2A event:\\n{topo2A_test[idx]}\\n')\n",
    "    print(f\"EB_test event:\\n{datasets['EB_test2']['L1_data'][idx]}\\n\")\n",
    "    print(f\"topo2A AD score held in datasets dict:\\n{datasets['EB_test2']['topo2A_AD_scores'][idx]}\")\n",
    "\n",
    "    # now run the event through the model\n",
    "    print(f'AD score calculate by running the event through the model:\\n{_AD_scores[i]}')\n",
    "\n",
    "# Train data ------------------\n",
    "print(f'\\n\\nStarting checks of test data!\\n\\n')\n",
    "\n",
    "# run these events through the model\n",
    "predictions = model.predict(topo2A_train[idxs])\n",
    "_AD_scores = np.mean(np.square(predictions), axis=1)\n",
    "\n",
    "for i, idx in enumerate(idxs):\n",
    "    print(f'\\n---\\nStarting check for index {idx}')\n",
    "    print(f'topo 2A event:\\n{topo2A_train[idx]}\\n')\n",
    "    print(f\"EB_test event:\\n{datasets['EB_train']['L1_data'][idx]}\\n\")\n",
    "    print(f\"topo2A AD score held in datasets dict:\\n{datasets['EB_train']['topo2A_AD_scores'][idx]}\")\n",
    "\n",
    "    # now run the event through the model\n",
    "    print(f'AD score calculate by running the event through the model:\\n{_AD_scores[i]}')\n",
    "\n",
    "\n",
    "# HLTnoAlgL1All data ------------------\n",
    "print(f'\\n\\nStarting checks of HLT_noalg_L1_all data!\\n\\n')\n",
    "\n",
    "# run these events through the model\n",
    "predictions = model.predict(Topo_2A_L1all[idxs])\n",
    "_AD_scores = np.mean(np.square(predictions), axis=1)\n",
    "\n",
    "for i, idx in enumerate(idxs):\n",
    "    print(f'\\n---\\nStarting check for index {idx}')\n",
    "    print(f'topo 2A event:\\n{Topo_2A_L1all[idx]}\\n')\n",
    "    print(f\"EB_test event:\\n{datasets['HLT_noalg_eb_L1All']['L1_data'][idx]}\\n\")\n",
    "    print(f\"topo2A AD score held in datasets dict:\\n{datasets['HLT_noalg_eb_L1All']['topo2A_AD_scores'][idx]}\")\n",
    "\n",
    "    # now run the event through the model\n",
    "    print(f'AD score calculate by running the event through the model:\\n{_AD_scores[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d198757-f78b-49f9-80ce-e30218edb098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c4c861f-0f72-4634-b76c-49dfe50368b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting checks of test data!\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "\n",
      "---\n",
      "Starting check for index 0\n",
      "topo 2A event:\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.09667968 -1.34375006 -3.37475777  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01215075  0.48962492]\n",
      "\n",
      "EB_test event:\n",
      "[[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [10.5        -2.16250014 -2.69980621]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 9.89999962 -2.1500001  -2.69980621]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 9.95389366  0.          0.39169994]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.0029272090177983046\n",
      "AD score calculate by running the event through the model:\n",
      "0.0029272090177983046\n",
      "\n",
      "---\n",
      "Starting check for index 2314\n",
      "topo 2A event:\n",
      "[ 0.29296875 -0.21875     0.30679617  0.20703126 -1.53125003 -3.86563152\n",
      "  0.18750001 -1.40625     2.02485457  0.18750001 -1.34375006  1.77941769\n",
      "  0.17773438 -0.15625    -1.53398082  0.15820313  0.96874997  2.2702916\n",
      "  0.12011719 -0.15625     0.30679615  0.09179687 -1.28125012  1.65669918\n",
      "  0.0625     -0.09375    -1.65669903  0.05078125 -0.34375001 -1.77941784\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01277488 -2.94524312]\n",
      "\n",
      "EB_test event:\n",
      "[[30.         -0.34999999  0.24543694]\n",
      " [21.20000076 -2.45000005 -3.09250522]\n",
      " [19.20000076 -2.25        1.61988366]\n",
      " [19.20000076 -2.1500001   1.42353415]\n",
      " [18.20000076 -0.25       -1.22718465]\n",
      " [16.20000076  1.54999995  1.81623328]\n",
      " [10.         -2.06250024  1.32535934]\n",
      " [ 9.39999962 -0.28750002  0.24543692]\n",
      " [ 4.69999981 -0.1875     -1.32535923]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [12.30000019 -0.25        0.24543692]\n",
      " [ 9.39999962 -2.05000019  1.32535934]\n",
      " [ 6.4000001  -0.15000001 -1.32535923]\n",
      " [10.4651804   0.         -2.3561945 ]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.005060174502432346\n",
      "AD score calculate by running the event through the model:\n",
      "0.005060174502432346\n",
      "\n",
      "---\n",
      "Starting check for index 132445\n",
      "topo 2A event:\n",
      "[ 0.38476564 -0.96874997 -1.04310691  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.20605469 -1.03125006 -1.1658255   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01613731  3.54274392]\n",
      "\n",
      "EB_test event:\n",
      "[[39.40000153 -1.54999995 -0.83448553]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [26.5        -1.63750005 -0.9326604 ]\n",
      " [ 3.0999999   0.36250001 -1.52170885]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [21.10000038 -1.6500001  -0.9326604 ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [13.21968269  0.          2.83419514]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.003926201723515987\n",
      "AD score calculate by running the event through the model:\n",
      "0.003926201723515987\n",
      "\n",
      "---\n",
      "Starting check for index -1\n",
      "topo 2A event:\n",
      "[ 3.90820324  0.71874999 -1.04310691  2.4589844   0.96874997  1.53398082\n",
      "  1.50390625  0.28124999 -3.74291301  1.37500003 -0.71874999  3.00660223\n",
      "  0.81054688 -0.09375     0.06135924  0.296875   -0.96874997  2.51572847\n",
      "  1.88281253  0.71874999 -1.04310669  1.31249994  0.96874997  1.53398082\n",
      "  0.78613281  0.21875001 -3.74291301  0.52539062 -0.71875006  3.00660223\n",
      "  1.25000006  0.25100388 -3.715958    0.62500003 -1.44493505 -2.25798145\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.09111516 -3.82640213]\n",
      "\n",
      "EB_test event:\n",
      "[[ 4.00200012e+02  1.14999998e+00 -8.34485531e-01]\n",
      " [ 2.51800003e+02  1.54999995e+00  1.22718465e+00]\n",
      " [ 1.54000000e+02  4.49999988e-01 -2.99433041e+00]\n",
      " [ 1.40800003e+02 -1.14999998e+00  2.40528178e+00]\n",
      " [ 8.30000000e+01 -1.50000006e-01  4.90873903e-02]\n",
      " [ 3.03999996e+01 -1.54999995e+00  2.01258278e+00]\n",
      " [ 1.60399994e+02  1.13750005e+00 -8.34485352e-01]\n",
      " [ 9.53000031e+01  1.56250000e+00  1.22718465e+00]\n",
      " [ 4.50000000e+01 -1.11250007e+00  2.40528178e+00]\n",
      " [ 8.00000038e-03  4.01606202e-01 -2.97276640e+00]\n",
      " [ 4.00000019e-03 -2.31189609e+00 -1.80638516e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.92800003e+02  1.14999998e+00 -8.34485352e-01]\n",
      " [ 1.34399994e+02  1.54999995e+00  1.22718465e+00]\n",
      " [ 8.05000000e+01  3.50000024e-01 -2.99433041e+00]\n",
      " [ 7.46415405e+01  0.00000000e+00 -3.06112170e+00]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.028722019866108894\n",
      "AD score calculate by running the event through the model:\n",
      "0.028722019866108894\n",
      "\n",
      "\n",
      "Starting checks of test data!\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "\n",
      "---\n",
      "Starting check for index 0\n",
      "topo 2A event:\n",
      "[ 0.18164063  1.97541714 -2.6874277   0.15234375 -0.78125    -1.77941769\n",
      "  0.1484375   1.40625    -0.1840777   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.08789062 -0.78125007 -1.77941784  0.05371094  1.09375     2.51572847\n",
      "  0.04882812 -1.34375006 -1.65669903  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01529342  1.59917444]\n",
      "\n",
      "EB_test event:\n",
      "[[18.60000038  3.16066742 -2.14994216]\n",
      " [15.60000038 -1.25       -1.42353415]\n",
      " [15.19999981  2.25       -0.14726216]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [10.60000038 -1.26250005 -1.42353427]\n",
      " [ 6.5999999   1.78750002  2.01258278]\n",
      " [ 3.5999999  -1.6875      1.71805847]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 9.         -1.25000012 -1.42353427]\n",
      " [ 5.5         1.75        2.01258278]\n",
      " [ 5.         -2.1500001  -1.32535923]\n",
      " [12.528368    0.          1.27933955]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.003316106041893363\n",
      "AD score calculate by running the event through the model:\n",
      "0.003316106041893363\n",
      "\n",
      "---\n",
      "Starting check for index 2314\n",
      "topo 2A event:\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.00775891 -3.64938855]\n",
      "\n",
      "EB_test event:\n",
      "[[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 4.         -1.1875      3.09250522]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 6.35609961  0.         -2.91951084]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.0013206707080826163\n",
      "AD score calculate by running the event through the model:\n",
      "0.0013206707080826163\n",
      "\n",
      "---\n",
      "Starting check for index 132445\n",
      "topo 2A event:\n",
      "[ 0.28710937  2.31137946  0.74095279  0.21679688 -0.90625003 -3.37475777\n",
      "  0.16796876 -1.74999997  0.6135923   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.10644531 -0.90625003 -3.25203925  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.46875    -0.81042126  3.91380519  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01987161 -3.80393326]\n",
      "\n",
      "EB_test event:\n",
      "[[ 2.93999996e+01  3.69820714e+00  5.92762232e-01]\n",
      " [ 2.22000008e+01 -1.45000005e+00 -2.69980621e+00]\n",
      " [ 1.72000008e+01 -2.79999995e+00  4.90873843e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.62999992e+01 -1.41250002e+00 -2.60163140e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.00000003e-03 -1.29667401e+00  3.13104415e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.08999996e+01 -1.45000005e+00 -2.60163140e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.62788200e+01  0.00000000e+00 -3.04314661e+00]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.005964773241430521\n",
      "AD score calculate by running the event through the model:\n",
      "0.005964773241430521\n",
      "\n",
      "---\n",
      "Starting check for index -1\n",
      "topo 2A event:\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00503309 2.46164843]\n",
      "\n",
      "EB_test event:\n",
      "[[0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]\n",
      " [4.12310553 0.         1.96931875]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.0015390323242172599\n",
      "AD score calculate by running the event through the model:\n",
      "0.0015390323242172599\n",
      "\n",
      "\n",
      "Starting checks of HLT_noalg_L1_all data!\n",
      "\n",
      "\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "\n",
      "---\n",
      "Starting check for index 0\n",
      "topo 2A event:\n",
      "[ 0.96874997  0.03125    -2.63844699  0.51562499 -0.28124999  1.53398082\n",
      "  0.2734375   0.84375001  0.55223309  0.25195312 -1.15625001 -1.04310691\n",
      "  0.20507812  0.34375001  2.14757308  0.16406249  0.53125001  2.51572847\n",
      "  0.45605469 -0.03125    -2.63844699  0.14550781 -0.34375001  1.4112623\n",
      "  0.12988281 -0.21875     1.65669918  0.10742188  0.03125     1.65669918\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.01916309  0.74383706]\n",
      "\n",
      "EB_test event:\n",
      "[[ 9.91999969e+01  5.00000007e-02 -2.11075759e+00]\n",
      " [ 5.27999992e+01 -4.49999988e-01  1.22718465e+00]\n",
      " [ 2.80000000e+01  1.35000002e+00  4.41786468e-01]\n",
      " [ 2.57999992e+01 -1.85000002e+00 -8.34485531e-01]\n",
      " [ 2.10000000e+01  5.50000012e-01  1.71805847e+00]\n",
      " [ 1.67999992e+01  8.50000024e-01  2.01258278e+00]\n",
      " [ 2.88999996e+01 -3.75000015e-02 -2.11075759e+00]\n",
      " [ 1.91000004e+01  3.75000015e-02 -2.11075759e+00]\n",
      " [ 1.25000000e+01  1.31250012e+00  4.41786468e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 4.67000008e+01 -5.00000007e-02 -2.11075759e+00]\n",
      " [ 1.48999996e+01 -5.50000012e-01  1.12900984e+00]\n",
      " [ 1.33000002e+01 -3.49999994e-01  1.32535934e+00]\n",
      " [ 1.56984072e+01  0.00000000e+00  5.95069647e-01]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.03648671135306358\n",
      "AD score calculate by running the event through the model:\n",
      "0.03648671135306358\n",
      "\n",
      "---\n",
      "Starting check for index 2314\n",
      "topo 2A event:\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  1.09375003 -0.77802531 -3.91376734  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.0240463  -0.90092525]\n",
      "\n",
      "EB_test event:\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 5.00000000e+00 -1.91250002e+00  2.40528178e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 7.00000022e-03 -1.24484050e+00 -3.13101387e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.96987305e+01  0.00000000e+00 -7.20740199e-01]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.0013949056155979633\n",
      "AD score calculate by running the event through the model:\n",
      "0.0013949056155979633\n",
      "\n",
      "---\n",
      "Starting check for index 132445\n",
      "topo 2A event:\n",
      "[ 1.16210938 -1.28124997  1.65669918  0.87890625  0.53125001 -2.2702916\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.67578122 -1.28125012  1.65669918  0.44921875  0.46875    -2.27029189\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.02125286 -1.92040905]\n",
      "\n",
      "EB_test event:\n",
      "[[119.          -2.04999995   1.32535934]\n",
      " [ 90.           0.85000002  -1.81623328]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [ 61.          -2.06250024   1.32535934]\n",
      " [ 48.40000153   0.78749996  -1.81623352]\n",
      " [  6.5999999   -1.98749995   1.42353415]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [  0.           0.           0.        ]\n",
      " [ 69.19999695  -2.05000019   1.32535934]\n",
      " [ 46.           0.75        -1.81623352]\n",
      " [  0.           0.           0.        ]\n",
      " [ 17.41034126   0.          -1.53632724]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.007631303276866674\n",
      "AD score calculate by running the event through the model:\n",
      "0.007631303276866674\n",
      "\n",
      "---\n",
      "Starting check for index -1\n",
      "topo 2A event:\n",
      "[ 2.17578128 -1.74999997  3.31339836  0.71484372 -0.15625     1.28854379\n",
      "  0.6660156   1.03124999  0.30679617  0.64453125 -1.34375006 -1.4112623\n",
      "  0.37890624 -0.65624997 -1.77941769  0.28710937 -0.34375001 -2.88388401\n",
      "  0.37304688  1.03124999  0.1840777   0.33007812 -0.15625     1.28854379\n",
      "  0.21484375 -1.34375006 -1.4112626   0.15917968 -0.34375001 -2.7611652\n",
      "  1.09375003 -0.77233903 -2.11102292  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.09023993 -1.26883104]\n",
      "\n",
      "EB_test event:\n",
      "[[ 2.22800003e+02 -2.79999995e+00  2.65071869e+00]\n",
      " [ 7.31999969e+01 -2.50000000e-01  1.03083503e+00]\n",
      " [ 6.81999969e+01  1.64999998e+00  2.45436937e-01]\n",
      " [ 6.60000000e+01 -2.15000010e+00 -1.12900984e+00]\n",
      " [ 3.87999992e+01 -1.04999995e+00 -1.42353415e+00]\n",
      " [ 2.93999996e+01 -5.50000012e-01 -2.30710721e+00]\n",
      " [ 2.40000000e+01 -2.87500024e-01  1.03083503e+00]\n",
      " [ 2.05000000e+01  1.61250007e+00  1.47262156e-01]\n",
      " [ 1.82999992e+01 -1.16250002e+00 -1.32535923e+00]\n",
      " [ 7.00000022e-03 -1.23574245e+00 -1.68881834e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.82000008e+01  1.64999998e+00  1.47262156e-01]\n",
      " [ 3.37999992e+01 -2.50000000e-01  1.03083503e+00]\n",
      " [ 2.20000000e+01 -2.15000010e+00 -1.12901008e+00]\n",
      " [ 7.39245529e+01  0.00000000e+00 -1.01506484e+00]]\n",
      "\n",
      "topo2A AD score held in datasets dict:\n",
      "0.010364829562604427\n",
      "AD score calculate by running the event through the model:\n",
      "0.010364829562604427\n"
     ]
    }
   ],
   "source": [
    "idxs = [0, 2314, 132445, -1]\n",
    "\n",
    "\n",
    "# Test data ------------------\n",
    "print(f'\\n\\nStarting checks of test data!\\n\\n')\n",
    "\n",
    "# run these events through the model\n",
    "predictions = model.predict(topo2A_test[idxs])\n",
    "_AD_scores = np.mean(np.square(predictions), axis=1)\n",
    "\n",
    "for i, idx in enumerate(idxs):\n",
    "    print(f'\\n---\\nStarting check for index {idx}')\n",
    "    print(f'topo 2A event:\\n{topo2A_test[idx]}\\n')\n",
    "    print(f\"EB_test event:\\n{datasets['EB_test2']['L1_data'][idx]}\\n\")\n",
    "    print(f\"topo2A AD score held in datasets dict:\\n{datasets['EB_test2']['topo2A_AD_scores'][idx]}\")\n",
    "\n",
    "    # now run the event through the model\n",
    "    print(f'AD score calculate by running the event through the model:\\n{_AD_scores[i]}')\n",
    "\n",
    "# Train data ------------------\n",
    "print(f'\\n\\nStarting checks of test data!\\n\\n')\n",
    "\n",
    "# run these events through the model\n",
    "predictions = model.predict(topo2A_train[idxs])\n",
    "_AD_scores = np.mean(np.square(predictions), axis=1)\n",
    "\n",
    "for i, idx in enumerate(idxs):\n",
    "    print(f'\\n---\\nStarting check for index {idx}')\n",
    "    print(f'topo 2A event:\\n{topo2A_train[idx]}\\n')\n",
    "    print(f\"EB_test event:\\n{datasets['EB_train']['L1_data'][idx]}\\n\")\n",
    "    print(f\"topo2A AD score held in datasets dict:\\n{datasets['EB_train']['topo2A_AD_scores'][idx]}\")\n",
    "\n",
    "    # now run the event through the model\n",
    "    print(f'AD score calculate by running the event through the model:\\n{_AD_scores[i]}')\n",
    "\n",
    "\n",
    "# HLTnoAlgL1All data ------------------\n",
    "print(f'\\n\\nStarting checks of HLT_noalg_L1_all data!\\n\\n')\n",
    "\n",
    "# run these events through the model\n",
    "predictions = model.predict(Topo_2A_L1all[idxs])\n",
    "_AD_scores = np.mean(np.square(predictions), axis=1)\n",
    "\n",
    "for i, idx in enumerate(idxs):\n",
    "    print(f'\\n---\\nStarting check for index {idx}')\n",
    "    print(f'topo 2A event:\\n{Topo_2A_L1all[idx]}\\n')\n",
    "    print(f\"EB_test event:\\n{datasets['HLT_noalg_eb_L1All']['L1_data'][idx]}\\n\")\n",
    "    print(f\"topo2A AD score held in datasets dict:\\n{datasets['HLT_noalg_eb_L1All']['topo2A_AD_scores'][idx]}\")\n",
    "\n",
    "    # now run the event through the model\n",
    "    print(f'AD score calculate by running the event through the model:\\n{_AD_scores[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c61489b-ece3-4565-bdd7-4b1eb6eedcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
